{
  
    
        "post0": {
            "title": "Using TF-Records on Spark Cluster",
            "content": "TF-Records abbreviated as TensorFlow Records is one the data formats that has serveral benefits such as performance (time) of the Tensorflow training. Using just TF-Records, I was able to get a direct decrease the training time 3x times. . Most of the example blogs on TF-Records however, had images as example. I was working on a usecase of training a tabular data on spark cluster. I am posting this blog for easing out TF-Records adoption for scenarios such as mine. Hope this is useful. . Note: The intention of this post is about how to use TF-Records for better performances. We could run the training distributed across the cluster.Tensorflow API supports distributed training. We could also try Horovod which uses Petastorm file format to train the data in the spark cluster. But those topics are beyond the scope of this post . Some Advantage of TF-Records are that, it . serialises data and stores. This means reduced space requirement for storage, faster data read and copy | uses Protocol Buffer format to store data which makes reading of data faster. | loads only required data into the memory. This is useful expecially for large datasets | The data is never brought to Python level and is always dealt with C++ level which makes training faster | Tensorflow moves the data to GPU while training is performed | . But to save/retrieve the records in TF-Records format, we need to have schema information to it which makes the process of creating TF-Records different from CSV / Excel / SQL. . Note: CSV/Excel doesn&#8217;t need even column names to both read and write. SQL doesn&#8217;t need datatype for querying however might need column name if we need only a specified column. This is not the case for TF Records. It need both name and its data-type for both write and read operations . Creating TF-Records . The first step is to write the data in the desired TF-Records format. The basic way to create TF-Records is to use tf.python_io.TFRecordWriter API. But there is a simpler way to create TF-Records in Spark cluster. . Library required . Creating the TF-Records in Spark cluster is easy. Thanks to the library spark-tensorflow-connector_2.11-1.10.0.jar. . Note: We are skipping the part on how we install this library into the cluster. This is a JAR install on the cluster and is usually generic. Also note that this library is needed only needed to create TF-Records. since we use Tensorflow (tf.data API) to read the data, we might not have to bother installing this library if we are using TF-Records created else where. . Writing the Tf-Records . The API is so neat and simple to create TF-Records. Since the schema can be infered from the dataframe itself, we need not provide the same to write the data. This might not be the case when we use the traditional TFRecordWriter. . Suppose you have a spark-dataframe preprocessed_df. The easiest way to create the TF-Records is : . preprocessed_df.write.format(&#39;tfrecords&#39;).save(path_to_save) . We could save the data in an actual spark table as an backup. . Tip: We might have to read the data from TF-Records for non tensorflow purpose as well (like data analysis). But it is typically slow to read the data (even using spark.read API) compared to saving it as spark table (as parquet files). So I have a copy of the data in spark table and one in TF-Records format. I am using the spark table that I use for analysis of data to infer schema for retrieving the TF-Records. If you dont like to dump the data, all you need is the list of columns and its type to decode the data in the end. . preprocessed_df.write.format(&#39;parquet&#39;).mode(&#39;overwrite&#39;).saveAsTable(preprocessed_table_name) . Reading TF-Records . To read the TF-Records for usage in tensorflow, we can use the tf.data API. . As discussed, TF-Records have a catch that to read the data from TF-Record files, we need to know the schema of the data to read/decode the files. Since we have spark table stored, we infer its schema from the spark table that we already saved. . Infering Schema . To read the records we need to have the list of features and their types, we could use the schema of the spark table. . So we just read the spark table that we already stored to infer its schema . preprocessed_df = spark.read.table(preprocessed_table_name) . We collect the schema dtypes as key/value pair into a dictionary . column_dtypes = {col:dtype for col,dtype in preprocessed_df.dtypes} . Create features from column dictionary . Now that we have the column dictionary, we have to create a features dictionary where we specify if the data is FixedLenFeature (typically for mandatory data) or VarLenFeature (typically for optional variables) and its data-type. I use FixedLenFeature as I dont have missing values . import tensorflow as tf # Since we have all fixed length features, we create a lambda helper function to create features _fixed_feature = lambda x: tf.io.FixedLenFeature([],x) def create_features(dtype_dict): features={} for dtype_tup in dtype_dict.items(): if dtype_tup[1] in (&#39;int&#39;,&#39;bigint&#39;,&#39;integer&#39;): features[dtype_tuple[0]] = _fixed_feature(tf.int64) elif dtype_tup[1] in (&#39;double&#39;,&#39;float&#39;,&#39;long&#39;): features[dtype_tuple[0]] = _fixed_feature(tf.float32) elif dtype_tup[1] in (&#39;string&#39;): features[dtype_tuple[0]] = _fixed_feature(tf.string) . Now that we have created the function, we can create features using that function create_feature . features = create_features(column_dtypes) . Decoding . Having created the features we could decode the TF-Records. To simplify the process we create a decode method. . Initially I used tf.io.Example API to decode. It down performed the training time as compared to training from CSV file, since it decodes one record at a time. . Important: If you used the common tf.io.Example API, you might face performance lag in decoding. This is because the data is decoded one record at a time. use of tf.io.parse_example will parse the data of the entire batch (refer the documentation for more understanding). It is to be noted that while decoding the serial data we must return a tuple of independent and dependent variables. But the data we stored doesnt have the knowledge of which columns constitute to both. Hence we might have to handle that in our decode method . output_cols={} # Specify the single or list of columns that are dependent variables def decode(serial_data): # We use `parse_example` instead of `Example` for decoding in batches parsed_data = tf.io.parse_example(serialized=serial_data,features=features) # We segregate the dependent variables seperately for returning the appropriate tuple y = {col:parsed_data.pop(col) for col in output_cols} return parsed_data, tf.transpose(a=list(y.values())) . Read data using tf.data . If we have created the data using spark, the files are put in the folder which we specify in the save method. This creates multiple TF-Record files. But it also contains the _SUCCESS file along with it. We might have to ignore that file while providing to the TF Data API to avoid a potential error. This can be done using a simple method as follows: . from pathlib import Path def get_tf_records(folder_path): return [i for i in map(str,Path(folder_path).iterdir()) if i.name != &#39;_SUCCESS&#39;] . Now, we can simply use this method to get all TF-Record files in the folder . training_records = get_tf_records(path_to_save) . The next part is to feed the data into the tf.data API. The API has a built-in method tf.data.Dataset.from_tensor_slices. . train_dataset = tf.data.Dataset.from_tensor_slices(training_records) . . Note: tf.data is an excellent API. It has lots of features that can improve the performance of the training. Some of the parameters like num_parallel_calls,block_length,cycle_length were useful in particular. The shuffle API allows us to provide a number of items to be picked up and shuffled to get batch_size items. The details on tuning tf.data for performance must be a post by itself. . Training the module . To train the module we need to: . Get the dataset | Batch | Decode | Prefetch (not mandatory, just a performance tweak) | . TF Data API has simplified this pipeline process by using chaining. So, we can simply fit like: . ... = model.fit(train_dataset.batch(batch_size).map(decode).prefetch(1),...) . Conclusion . Along side TF-Records once the TF Data API is tuned we got performance improvement around 3.75-4x times. Just plain TF-Records was providing plain 3x times performance improvement on tabular data. . We could see blogs such as these by Sebastian Wallkötter which claim 7x improvement in Image dataset. . Based on my understanding, the reasons on why I couldnt achieve that improvement are as follows: . Data was stored in SSD (both CSV and TF-Records) which by itself does faster reads. Hence the impact of TF-Records being read faster became less predominant | Training time for tabular batch are typically slow compared to image data as tabular data has fewer features. (making our performance bottle neck CPU bound) | . We could try to distribute training using a distribution strategy available in tensorflow in spark. Before distribution, we can consider if the issue is CPU/GPU bound (i.e) the performance bottle neck was on the time taken to read the data or train the model. . For instance, GPU which does the training might be waiting for the CPU to get the training data for each batch, if the training finishes before retrieving the data for the next batch. This makes the performance bottle-neck CPU bound. . If the training take more time than the time to retrieve the data, we might have CPU wait for GPU to complete the training. This makes the problem GPU bound. . This idea of wheather an issue is CPU or GPU bound gives us idea on what further course of action can be done to improve the performance. . Useful Sources: . Spark TensorFlow Connector | Official Documentation | TF Records Good on Keras TensorFlow discussion on Quora | 7x speedup with an optimized TensorFlow Input pipeline: TFRecords + Dataset API | Tensorflow Records? What they are and how to use them | . Kindly share your experience and perspectives on training with TF-Records and on spark clusters .",
            "url": "https://nareshr8.github.io/ml_posts/tf-records/spark/py-spark/performance/2020/07/01/TF-Records-on-Spark.html",
            "relUrl": "/tf-records/spark/py-spark/performance/2020/07/01/TF-Records-on-Spark.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "X-Ray Diffractions Detector",
            "content": "This program is try to do something that I have no idea to detect if it is even distantly correct. I was able to find a peculiar dataset called X-Ray Defraction Imageset thanks to Czyzewski et. al. The imageset was downloaded for traning the dataset from the link available here. I love to try out something new as a hobby and see how it works out to be. . Czyzewski, Adam, Krawiec, Faustyna, Brzezinski, Dariusz, &amp; Porebski, Przemyslaw J. (2019). RefleX: X-ray diffraction images dataset (Version 1.0.0) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.2605120 . X-Ray Defraction Imageset from here . Tracking Experiments . Now we are going to explore options on how to get our accuracy high enough. We have various tools that track the experiments that were done. Having such things help us to understand what hyper parameters to choose and get intutions on what experiments to try. . We are going to use WandB which is one amoung the top tools used. . I may use this for klogging my experiments. However, Since the one that gave the best result is shown below, its trivial if you use WandB and WandBCallback. . #collapse import os os.environ[&#39;WANDB_BASE_URL&#39;] = &#39;http://localhost:8080&#39; from fastai2.callback.wandb import wandb,WandbCallback . . Extract Data . If you havent extracted the zip file, the first step might be to extract it. The extraction can be manual or can be done using Python . Setup the notebook . The first step might be to setup the notebook and set path correctly. This will remove all possible hindrences on later. . Lets just import the required libraries from fastai2. As it is a vision task we shall try importing fastai2.vision . Also the file that maps the labels to the images is a CSV. we know pandas is good at tabular data and is the most convinent way to handle CSV data. So, we include that as well . from fastai2.vision.all import * from fastai2.vision.widgets import * import pandas as pd . We create path variable to be pointing to the location wherever it is downloaded . #collapse path=Path(&quot;../../data/Multi-label/&quot;) Path.BASE_PATH = path . . Path from pathlib by itself doesn&#39;t have a convenience function to list all child files/folders. we may have to actually do list(path.iterdir()) to get that. Fast-AI however has made a convenient function .ls() that gets overloaded as we import the library . files=path.ls() files . (#3) [Path(&#39;images&#39;),Path(&#39;labels.csv&#39;),Path(&#39;reflex_img_1024_inter_nearest.zip&#39;)] . Here we see that the images are available in the images folder and the label-image mapping is done in labels.csv. The original downloaded zip file is the other reflex_img_1024_inter_nearest.zip file that is available. However it is never used in this program . Analysis . I have no idea how the data is in the CSV or image. Lets just load the data and have a look at it . labels_df=pd.read_csv(path/&#39;labels.csv&#39;) labels_df . image loop_scattering background_ring strong_background diffuse_scattering artifact ice_ring non_uniform_detector . 0 152803_2_0001 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | . 1 81258_1_E1_001 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | . 2 HMCb11_AY6-7_00001 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | . 3 195706_1_E2_00001 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . 4 nui1-10_1_001 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 6306 wjm1-6 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 6307 xtal-C7-eg.0001 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | . 6308 AR1_IF0130_screen_0001 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | . 6309 215574f10_x0001 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | . 6310 63667_1_E2 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 6311 rows × 8 columns . Now we see that the CSV has image as the first column and columns that representing the label as list of columns each reflecting 1 or 0 as in True or False. . There are totally 6311 samples. We also see that the number of columns available are 8. 1 input (Image file name) and the 7 probable classes. . Data Munching . All the label columns are having values as 0 or 1. This is one of the ways to directly feed this as on-hot encoded vector to the model. But we may have inconvinence in interpreting 0 and 1 and mapping columns. . If we let fastai2 do the one-hot-encoding, we may have some extra convience in interpreting the model output. So I prefer to have the labels as the actual string with comma seperation between those columns. I do that in the following steps. . Get all label columns | For each row, if the label is marked as present (i.e) value is 1, then add that to the string | Add a new column labels with the label names of the ones that just exist | . y_cols=list(labels_df.columns[1:]) . def get_labels(row): return &#39;, &#39;.join([col for col in y_cols if row[col]==1]) . labels_df[&#39;labels&#39;]=labels_df.apply(get_labels,axis=1) . We have made up a new column labels which lists the labels for that image. Lets see if we got it right . labels_df . image loop_scattering background_ring strong_background diffuse_scattering artifact ice_ring non_uniform_detector labels . 0 152803_2_0001 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | loop_scattering, strong_background | . 1 81258_1_E1_001 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | loop_scattering, background_ring, strong_background | . 2 HMCb11_AY6-7_00001 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | background_ring, strong_background, artifact | . 3 195706_1_E2_00001 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | artifact, ice_ring | . 4 nui1-10_1_001 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | loop_scattering, background_ring, strong_background | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6306 wjm1-6 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | background_ring | . 6307 xtal-C7-eg.0001 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | non_uniform_detector | . 6308 AR1_IF0130_screen_0001 | 0 | 0 | 1 | 0 | 1 | 0 | 1 | strong_background, artifact, non_uniform_detector | . 6309 215574f10_x0001 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | loop_scattering, background_ring, strong_background | . 6310 63667_1_E2 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | loop_scattering, ice_ring | . 6311 rows × 9 columns . There are 6311 rows. which means there should be atleast 6311 images in the file . len(Path(path/&quot;images&quot;).ls()) . 6312 . We have one more than required. Lets see if we have any non .png file or folders . Path(path/&quot;images&quot;).ls().filter(lambda x: x.suffix!=&#39;.png&#39;) . (#1) [Path(&#39;images/models&#39;)] . Ok. There seems to be an empty models folder present in images folder. . Ok. Now lets look at random image . from random import randint total_count=len(Path(path/&quot;images&quot;).ls())-1 # Discount the models folder index= randint(0,total_count) image_paths=Path(path/&quot;images&quot;).ls().filter(lambda x: x.suffix==&#39;.png&#39;) image_path=image_paths[index] im = Image.open(image_path) im.thumbnail((100,100)) im . Rerunning the above cell provides enough samples to get an idea of the dataset. Seems the dataset is grey scale . Preparing the data . We need DataLoaders to point out the list of transformations that are to be done to the data for preparing for training. Luckily, we have ImageDataLoaders.from_df method to get an handy function for generating the dataloader. But it doesnt have provision to provide a uni channel Image. So we do some edit of the same method. . There are couple of things to note on the decision of parameters: . First parameter is the dataframe itself -label_col is the column which has the list of labels | path must specify the path where the images are stored | suff is the suffix that must be added to all the filenames. In short the file extensions | label_delim specifies the delimiter that segregates two labels apart (in the label_col) | item_tfms specifies the item level transformations that must be done. Here we are just doing resizing of original image to 224*224 image size. The idea of using Squish to resize was because I am guessing cropping might remove some of the features. Especially after looking at some label names like loop_scattering , non-uniform_detector or strong_background. loop_scattering is fairly obvious there seems to be some disturbance in the rings which is that that obvious in the inner rings. cropping might not be that useful. . | . Note:The Default augmentations are not added on purpose as brighness, contrast can affect the strong_background feature . def generate_dataloaders( df, path=&#39;.&#39;, valid_pct=0.2, seed=None, fn_col=0, folder=None, suff=&#39;&#39;, label_col=1, label_delim=None, y_block=None,x_block=None, valid_col=None, item_tfms=None, batch_tfms=None, **kwargs): &quot;Create from `df` using `fn_col` and `label_col`&quot; pref = f&#39;{Path(path) if folder is None else Path(path)/folder}{os.path.sep}&#39; if y_block is None: is_multi = (is_listy(label_col) and len(label_col) &gt; 1) or label_delim is not None y_block = MultiCategoryBlock if is_multi else CategoryBlock if x_block is None: x_block=ImageBlock splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col) dblock = DataBlock(blocks=(x_block, y_block), get_x=ColReader(fn_col, pref=pref, suff=suff), get_y=ColReader(label_col, label_delim=label_delim), splitter=splitter, item_tfms=item_tfms, batch_tfms=batch_tfms) return DataLoaders.from_dblock(dblock, df, path=path, **kwargs) . We now create the dataloader with this data frame: . x_block might be the black and white monochrome image. | path specifies the location of images | label_col specifies the column where the dependent variable is available. | label_delim soecies the delimiter that splits the labels available in label_col | suff adds the suffix to the image. | item_tfms specifies the transformation to be done at item level. Here we resize to a 48*48 image by squishing so that the entire image is given | bs specifies the batch_size to be used | . dataloaders=generate_dataloaders(labels_df,x_block=ImageBlock(PILImageBW),label_col=&#39;labels&#39;,path=path/&#39;images&#39;,suff=&#39;.png&#39;,label_delim=&#39;, &#39;,item_tfms=Resize(48,ResizeMethod.Squish),bs=128) . Lets get a batch of data and see if we have a have the shape of input as we expect. . dataloaders.one_batch()[0].shape . torch.Size([128, 1, 48, 48]) . The default bs being 128, images having 1 channels after resizing the image to 48*48 the shape seem reasonable . Let&#39;s see some random image out of it and see if we have everything right . dataloaders.show_batch(max_n=9) . Training . Now we use a pretrained Resnet18 and use the train for this perticular dataset . . Note: We are using the pretrained Resnet18 via Transfer Learning, which basically means that we are using a model that is already trained for doing good at something else (which is to predict classes in ImageNet here) to do our task. This reduces the traning time. . learn = cnn_learner(dataloaders, resnet18, metrics=partial(accuracy_multi, thresh=0.5), cbs=WandbCallback()) . lr_find is an useful functionality that is useful in finding optimal learning rqate for training. It outputs the graph and the two values onw which specifies the lr where the loss was at its least and other one at the steepest slope. The steepest slope is the interest for us for most of the cases . learn.lr_find() . SuggestedLRs(lr_min=0.014454397559165954, lr_steep=0.03981071710586548) . fine_tune tunes the newly introduced layer for one epoch and unfreeses the previous layers for training for the later epochs. We would see two tables representing its loss and other metrics for the same reason . learn.fine_tune(10, 4e-2) . wandb: Wandb version 0.8.34 is available! To upgrade, please run: wandb: $ pip install wandb --upgrade . epoch train_loss valid_loss accuracy_multi time . 0 | 0.495263 | 0.373240 | 0.831220 | 00:22 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.349745 | 0.311832 | 0.872538 | 00:23 | . 1 | 0.338403 | 0.339598 | 0.850351 | 00:24 | . 2 | 0.339028 | 0.308806 | 0.875481 | 00:23 | . 3 | 0.318818 | 0.334795 | 0.861444 | 00:23 | . 4 | 0.294721 | 0.269664 | 0.890424 | 00:22 | . 5 | 0.254378 | 0.249635 | 0.901064 | 00:23 | . 6 | 0.220228 | 0.236530 | 0.910233 | 00:23 | . 7 | 0.177766 | 0.221808 | 0.916459 | 00:22 | . 8 | 0.150120 | 0.213772 | 0.923251 | 00:22 | . 9 | 0.131988 | 0.215067 | 0.923817 | 00:23 | . So, I tried running through various experiments. The code below helps in loading the previous saved model. I save models which have better accuracies and use it as a starting point for going further. . learn=learn.load(run_name+&#39;.h5&#39;) . For this experiment I didnt change architecture much. I tried changing the image size and batch_size and learning_rate. Choosing the learning rate after initial training is little hard. Thanks to W&amp;B I could see the learning rate for various epochs. I chose the lr which had the slopiest accuracy improvement at the end of the training and updated it . learn.fit_one_cycle(10,1e-6) . wandb: Wandb version 0.8.34 is available! To upgrade, please run: wandb: $ pip install wandb --upgrade . epoch train_loss valid_loss accuracy_multi time . 0 | 0.047517 | 0.220021 | 0.932194 | 00:22 | . 1 | 0.048205 | 0.219809 | 0.931515 | 00:22 | . 2 | 0.047783 | 0.223721 | 0.931062 | 00:22 | . 3 | 0.047295 | 0.223825 | 0.929477 | 00:22 | . 4 | 0.045167 | 0.222147 | 0.930383 | 00:22 | . 5 | 0.043862 | 0.222521 | 0.931288 | 00:22 | . 6 | 0.045675 | 0.222911 | 0.928911 | 00:22 | . 7 | 0.044050 | 0.223413 | 0.929364 | 00:22 | . 8 | 0.046282 | 0.224207 | 0.930722 | 00:23 | . 9 | 0.045452 | 0.220096 | 0.930156 | 00:23 | . We would have to save the model for later inference . learn.save(run_name+&#39;.h5&#39;) wandb.save(run_name+&#39;.h5&#39;) . [] . Results . Of some of the experiments we did, we saw the results as follows: . Run name Image Size Performance Batch Size . Basic Resnet 18 | 224 | 0.941193 | Default | . Basic Resnet 18 - 48 (256) | 48 | 0.926874 | 256 | . Basic Resnet 18 - 48 (128) | 48 | 0.938307 | 128 | . Basic Resnet 18 - 48 (64) | 48 | 0.936382 | 64 | . Basic Resnet 18- 48 (32) | 48 | 0.931741 | 32 | . Which Model to choose: . Of some of the various experiments trained, we didnt include any of them when them were we used 3 channels as input simply because it gave the same result but occupying more weights for model anf might have slightly more time to train for infer. . Note: One of the decision to take is between Run 1 and Run 3. . Run 1 has around 0.3% more accuracy than Run 3 but it uses bigger image. This means: . Model of Run 1 is bigger than Run 3 | Run 1 might take more time to train or infer than Run 3 . Tip: If the usecase allows a slightly accurate model always prefer the smaller model | . Prediction . X-Ray images are less popular. So I am choosing a image from the validation set to check how it is predicting. To do so, first lets look at validation image. . learn.dls.valid_ds.items[[&#39;image&#39;,&#39;labels&#39;]] . image labels . 3762 49967_1_E2_001 | background_ring, diffuse_scattering | . 4146 9172_1_E1_001 | loop_scattering, background_ring, non_uniform_detector | . 6190 ATPM1E10gly-peak.0001 | background_ring, non_uniform_detector | . 1845 15533_1_E2_001 | loop_scattering, strong_background | . 4903 r9_3.0001 | background_ring | . ... ... | ... | . 2584 120919_1_E1_001 | loop_scattering, strong_background | . 2800 93400_1_E2_001 | loop_scattering, background_ring, non_uniform_detector | . 1869 64132_2 | background_ring, diffuse_scattering | . 2555 K16_ok2 | | . 5205 foj9-16_4 | loop_scattering, background_ring, ice_ring | . 1262 rows × 2 columns . I am choosing the second one as a random image to test . learn.predict(path/&#39;images/9172_1_E1_001.png&#39;) . ((#3) [&#39;background_ring&#39;,&#39;loop_scattering&#39;,&#39;non_uniform_detector&#39;], tensor([False, True, False, False, True, True, False]), tensor([6.4019e-03, 9.9865e-01, 3.6687e-02, 7.4511e-06, 9.9855e-01, 9.6328e-01, 6.6407e-04])) . Here it provides the Predicted labels, The One Hot Encoding values infered and the actual tensor values of the model. . We also see that this label prediction is indeed correct . Conclusion . The Idea of this blog is to go through a simple Multi-Label Image Classifer. Though there are lots of other options that could have experimented with, since the dataset was relatively simple, we leave them here. . We can try a different dataset which demands more accuracy in a different blog post . Kindly let know your comments on the same .",
            "url": "https://nareshr8.github.io/ml_posts/image-classifer/fastai2/2020/04/30/Multi-Label-Image-Classification.html",
            "relUrl": "/image-classifer/fastai2/2020/04/30/Multi-Label-Image-Classification.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Classical Dances of India",
            "content": "There are various classical dances that are available all throughout in India. These classical dances are culture specific. For example, Tamil Nadu follows a classical dance called Bharathanaatiyam where as its nearby state, Kerala has a classical dance of Kathakali. . Being not much knowledgable in any of the classical dances, I was triggered by the idea of Jeremy Howard who was suggesting that we might be able to solve deep learning problems even if we ourselves arent the domain experts. So, I decided to try myself out if I could build a model that can perform well in classifying the classical dance names. . I have already done the same. But this time on the latest version of FastAi (V2) and is part of the ongoing course on it which is going to be public by June 2020. If you have checkd out the previous blog post on this, You may consider this as just a version updated blog. . Steps . To build a classifier, we have to build a classifier on our own that can be run end to end, we need to follow the below steps. . Download the dataset | Do preprocessing of data, if any | Create a model to detect the classical dance | Deploy it using a single GUI | . Downloading the Dataset . To download a dataset, we need some Image Search API. Bing is one such API which provides better image downloading capabilities. But we may have to register to Bing Image Search to get the access key to use the API. . from azure.cognitiveservices.search.imagesearch import ImageSearchClient as api from msrest.authentication import CognitiveServicesCredentials as auth from fastai2.vision.all import * from fastai2.vision.widgets import * def search_images_bing(key, term, min_sz=128,count=150): client = api(&#39;https://api.cognitive.microsoft.com&#39;, auth(key)) return L(client.images.search(query=term, count=count, min_height=min_sz, min_width=min_sz).value) . after signing up, add the key in the below cell and list the classes that you want to classify . key=&#39;xxx&#39; classes = [&#39;Bharathanatyam&#39;,&#39;Kathakali&#39;,&#39;Kathak&#39;,&#39;jagoi dance&#39;] . Select a path where you would love to do image classification . path=Path(&#39;./data/image-classification&#39;) . Download images for each class and put it in a different folder where the folder name specifies the class name . path.mkdir(exist_ok=True) for o in classes: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, o) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . After downloading images, verify if the files are generated as appropriate . fns = get_image_files(path) fns . (#591) [Path(&#39;data/image-classification/Bharathanatyam/00000080.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000001.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000125.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000062.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000124.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000015.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000144.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000033.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000140.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000071.jpg&#39;)...] . Now that we have downloaded the images. Our next step is to train the model to learn identifying classical dance images. . Look into the images . After we downloaded the images, we need to check if the downloaded images have some corrupt images as well. These images will not open for some reason and hence break when we try to open. We dont want to train our network with these images. . Fast AI comes with a method to verify these images using verify_images method . failed = verify_images(fns) failed . (#2) [Path(&#39;data/image-classification/Kathak/00000069.jpg&#39;),Path(&#39;data/image-classification/Kathak/00000117.jpg&#39;)] . Delete the corrupted images by using unlink method in Path . failed.map(Path.unlink) . (#2) [None,None] . Creating DataBlock . Creating a datablock is the next step. DataBlock is the place where we specify essentially everything about the data that has to be done before giving it to the model. . classical_dances_db = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128)) . In the above code: . blocks is used to specify the way to generate Dependent and independent variables | get_items provides a way to get each item from the path. In this case, get individual image paths as files | splitter provides a way to split validation and training data. We have made sure the same set of images are used evertime we randomly divide the dataset into training and test set. | get_y a method to get Y value out of individual path value. Since getting Y from the path is little tricky i.e looking at the parent folder name, we specify the way to get y | item_tfms specifies transformations that are to be doneat item level. Such as resizing to 128*128 image (usually done through center cropping) | . There are some transforms like moving to GPU, Normalisation that are done implicitly. . Now we specified the shell of the data, we now load the data to be used for training using dataloaders. The dataloaders convinently converts all the data that are provided and do all the transformations to be done so that the data can be fed into the model for training. . dls = classical_dances_db.dataloaders(path) . Now that we have transformed the data, we might look at the data if it is transformed right. . dls.show_batch(max_n=4, nrows=1) . Now after some analysis we find that these images like ok. So, lets start by looking into training the machine . Training the model . Train the model with ResNet-18 and finetune 4 epochs . As we are all set for training the machine, we can continue to train the machine learn the type of dance from image . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.740276 | 2.458594 | 0.522727 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.900825 | 0.744960 | 0.244318 | 00:04 | . 1 | 0.693868 | 0.367972 | 0.102273 | 00:04 | . 2 | 0.556724 | 0.302496 | 0.102273 | 00:04 | . 3 | 0.457315 | 0.283369 | 0.096591 | 00:04 | . Here we created a cnn_learner which takes: . dls - The data to be loaded | arch - Pretrained Resnet Model (resnet18) so that we can ran faster | metrics - The metrics to be displayed after training | . Understanding the model . we will analyse the model so that we can get enough information out of it . Confusion Matrix is a matrix which tabulates the Actual Vs Predicted output. If both are same, then out machine did great job in identifying the image. Lets see how our model preformed . Look at the confusion matrix to see which one goes wrong mostly . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . The confusion is mostly around Kathak. The model gets wrong on Kathak often with Bharathanatyam and jagoi dance. . Look at the most confused images . interp.plot_top_losses(4, nrows=2) . Cleanup . There can be cases where some images are messy. Like, The search result which has a dancer who is famous in, say Bharathanatyam performing Kathak. These images can confuse the model in learning. Lets look if thats the case . This is also a place where we have to probably consult people who are data experts. They might help us to know if the image we call as Kathak were actually of Kathak Dance per say. . FastAI comes with this very cool widget which is very useful in these cases . cleaner = ImageClassifierCleaner(learn) cleaner . The output of the previous one is an interactive widget which allows you to select images that were wrongly classified or images that are to be deleted. . Some common images that had to be removed includes: . Photos of probably famous dances that were taken during some interview or award ceremony | Agenda of the dance festival | The ornaments and dresses thats associated with that dance | The image of artist while they do the make over or dressing while preparing for the dance. | . We had Images one of each type. Hence we do the appropriate using change and delete methods. . cleaner.change() cleaner.delete() . (#1) [7] . Rerunning the model after cleaning the data . Since the mode is rerun, the model might have a better at understanding now since the confusing images are removed. So, We are essentially repeating things that we were doing earlier . classical_dances_db_2 = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = classical_dances_db_2.dataloaders(path) . dls.show_batch(max_n=4, nrows=1) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.949091 | 1.575333 | 0.369318 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.945574 | 0.684837 | 0.238636 | 00:04 | . 1 | 0.711181 | 0.396144 | 0.119318 | 00:04 | . 2 | 0.528489 | 0.341853 | 0.090909 | 00:04 | . 3 | 0.423833 | 0.317606 | 0.102273 | 00:04 | . learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 0.113944 | 0.557393 | 0.096591 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.058908 | 0.532337 | 0.113636 | 00:04 | . 1 | 0.060650 | 0.552075 | 0.125000 | 00:04 | . 2 | 0.040418 | 0.566424 | 0.119318 | 00:04 | . 3 | 0.031581 | 0.566204 | 0.102273 | 00:04 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Add Augumentations . We do Data Augumentations, as this is a very basic work, we try to have default augumentations in aug_tfms. and try the process one more time . classical_dances_db_3 = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128), batch_tfms=aug_transforms()) . dls = classical_dances_db_3.dataloaders(path) . dls.show_batch(max_n=4, nrows=1) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.992708 | 1.989776 | 0.454545 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.966205 | 0.912835 | 0.278409 | 00:04 | . 1 | 0.805900 | 0.455987 | 0.147727 | 00:04 | . 2 | 0.669330 | 0.356659 | 0.113636 | 00:04 | . 3 | 0.567247 | 0.332295 | 0.107955 | 00:04 | . learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 0.317089 | 0.318796 | 0.107955 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.392914 | 0.311143 | 0.096591 | 00:04 | . 1 | 0.337256 | 0.339022 | 0.113636 | 00:04 | . 2 | 0.318557 | 0.297601 | 0.096591 | 00:04 | . 3 | 0.292514 | 0.286991 | 0.090909 | 00:04 | . interp.plot_top_losses(4, nrows=2) . Export the model . There is obviously more things that can be done here. But with some good defaults, we get around 91% accuracy. Since we are satisfied with them, lets export the model by calling learn.export() . learn.export() . Loading and Predicting the model . path = Path(&#39;/&#39;) path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . We can load the model using load_learner method. . learn_inf = load_learner(path/&#39;export.pkl&#39;) . Now we can load a random image. It can be outside our dataset. Just for ease I am choosing one from the dataset itself. . path=Path(&#39;./data/image-classification&#39;) . paths=(path/&#39;Bharathanatyam&#39;).ls() . paths[0] . Path(&#39;data/image-classification/Bharathanatyam/00000080.jpg&#39;) . Now we predict by calling the predict method passing in the path of the image . learn_inf.predict(str(paths[0])) . (&#39;Bharathanatyam&#39;, tensor(0), tensor([9.9981e-01, 8.2153e-05, 8.2317e-05, 2.8997e-05])) . As you can see, the file was in the Bharathanatyam folder and the prediction was also Bharathanatyam with the confidence of 99.98%. Hence we got the correct prediction . If you have a real world data and see how it is performing with your set of images, test it here. . It might take some time to start the app as we are using binder to get it running though .",
            "url": "https://nareshr8.github.io/ml_posts/image-classifer/fastai2/2020/04/02/Image-Classification.html",
            "relUrl": "/image-classifer/fastai2/2020/04/02/Image-Classification.html",
            "date": " • Apr 2, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding Linear Regression",
            "content": "The linear regression is the basic building block algorithm for most Machine learning algorithms. It is the simplest machine learning algorithm used for both regression and classification problems. It is used as a building block for most complicated architectures like Neural Network, Convolution Neural Network and Recurrent Neural Network. Hence having a basic understanding for it will give us a great overview of how complicated machine learning architectures are built upon. . Basics . For Machine Learning to happen, we need 3 groups of datasets, synonymous to a student learning maths: . Training Set: The data that we use to teach the machine (Text Book questions) | Test Set: The data that we use to test if it understood correctly (The Exam Paper) | Real World Set: The Real world example of using the learned skill. | . Lets take an example of the training set say centuries vs auction price. Its fairly obvious that if you have scored more centuries, you are a great player and hence you&#39;ll be bought for more price. Lets see how the machine learns this fact and how it could predict the auction price. . Centuries Auction Price . 2 | 200,000 | . 3 | 330,000 | . 5 | 450,000 | . 6 | 610,000 | . 7 | 750,000 | . Now we are making our student (Machine) understand the above table. But how could we do that. Lets see.. . Little Bit of Math &#128530; . Let&#39;s take Centuries scored as the x variable and Auction Price as y variable or the variable to be predicted. The price of a player will be based on the number of centuries, they scored. However, there is a fair say that there would be a base price even which doesn&#39;t depend on the number of centuries. . Lets say that we would like to fit this to a variable begin{equation} y=a*x+b end{equation} . Where the value of a and b are weights of centuries and base price that could affect the auction price and our task is to find this weight-age. . So for example i, we have the equation as follows: begin{equation} y_i=a*x_i+b end{equation} . So, if we have to calculate values for i examples of $x_i$ and $y_i$ variables, we need to do the computation in loop. . Now we can also consider $y_i=a*x^1_i+b*x^0_i$ . Let&#39;s call the complete vector of $x$ ($x^1$ and $x^0$) for all $i$ examples to be $X$ . Complete vector of $y$ (for $i$ examples) to be $Y$. Now the equation becomes like this. . begin{equation} Y=X.A end{equation}where begin{align*} A = begin{bmatrix} a b end{bmatrix} end{align*} . As we consider that the value b is multiplied by $ x^0 $ (which is 1). So, begin{align*} X = begin{bmatrix} x_0 x^0_0 x_1 x^0_1 vdots x_n x^0_n end{bmatrix} end{align*} . and begin{align*} Y = begin{bmatrix} y_0 y_1 vdots y_n end{bmatrix} end{align*} where the subscript represents the example number (1 through n) . The Shape of Y is $n*1$ and $X$ is $n*2$, where n represents the number of examples. . We can see a working demo of how this equation sums up here. . Finding the Coefficients . Now we know what is X and Y. But, we don&#39;t know what is A (i.e) a and b. We are going to use Stochastic Gradient Descent to identify them. . We can use Pytorch for finding this. . Since FastAI uses Pytorch and has some additional capabilities, we can import this one which internally imports Pytorch. So, lets import the library here. . %matplotlib inline %matplotlib nbagg from fastai.basics import * . Now that we have imported the library, lets create some random dataset. . Note: We could use the dataset in the table above, but it is very small amount of data as compared to what we need. So lets generate some random data . n=100 . Now for the 100 records, we are creating a matrix of 2 dimensions, n representing the number of records and 2 representing each example having 2 values. One value being dynamic $x^1$ and other being $x^0$ (i.e. 1). Now $x^1$ may have varied values, however $x^0$ will always be one. . So, we . create a 2D matrix of $100*2$ values of all ones | Replace the first column with values from a uniform distribution $[-1,1)$ where -1 is included and 1 is excluded | . We then verify if the value got updated properly. . X= torch.ones(n,2) X[:,0].uniform_(-1,1) X[:5] . tensor([[ 0.7498, 1.0000], [ 0.2281, 1.0000], [-0.5831, 1.0000], [ 0.4266, 1.0000], [-0.2114, 1.0000]]) . We then create a tensor with 2 values that could represent A. We arbitrarily set a random value. Lets take it as 134112 and 220. where $a=134112$ and $b=220$. This is done just to create a dataset with some values. But in real world scenario, $A$ is the value that we would have to find. . A=tensor([134112.,220]) . Now, we are going to do the matrix product of the values X and A to get our Y value. Note that we are adding a random value here so that we dont have . Y=X@A+((torch.rand(n)*100000)); . Lets see how it looks in a graph . plt.scatter(X[:,0], Y) . &lt;matplotlib.collections.PathCollection at 0x7fbc70166cc0&gt; . Now, our goal is to find the correct value of begin{align*} A = begin{bmatrix} a b end{bmatrix} end{align*} so that we can predict $Y$ for any new set of examples $X$. To do that we have to calculate something called error. . Error . Now lets look at what values we could get for a random value of A. A = $ begin{align*} begin{bmatrix}4112. 22000 end{bmatrix} end{align*}$ gives the value as below. . A=tensor([-41289.,22000]) plt.scatter(X[:,0], Y) plt.plot(X[:,0], X@A,c=&quot;orange&quot;) plt.xlabel(&#39;Centuries&#39;) plt.ylabel(&#39;Auction Price&#39;) . Text(0,0.5,&#39;Auction Price&#39;) . Now we can clearly see that The Orange color line is way to far from actually predicting the auction price. Now, we have to define far. This is where error comes in. We want to give a number to how far it is from the actual result. . Classical Error function . Now, just by looking into the data we would say the error is the difference between the actual value versus the predicted value. Well, That can be one of the error function. This can look like this: . $J(a,b) = sum_1^n(y- hat{y} )$ . where J denotes the cost function, $a$ denotes the parameters which fit in A. . However, In practice we are using a different cost function (Note that error function and cost function dentotes the same). This is because if we magnify the error function a little bit, we converge to the correct value more than stating the actual value, using gradient descent. (This is proven to work in machines as well..). So, we take the error function as Mean Squared Error . Mean Squared Error . Mean squared error basically takes the square value of the difference. So, The loss function changes to a little bit different. . $J(a,b) = sum_1^n(y- hat{y})^2$ . So, we can create the function J as follows: . def mse(y_hat,y): return ((y_hat-y)**2).mean() . Optimisation . Now that we have defined the error function, we are trying to define a way to optimize the value of A such that we reduce the error, intern predicting the value of $ hat{Y}$ (predicted value) closest the actual $Y$ (actual value). Gradient Descent is an algorithm which is used to reduce the value of a given function. The way it works is by taking the initial set of parameters $A$ and iteratively optimizing the value to minimize the function. . This iterative minimisation is achieved by taking individual steps in the negative direction of the gradient function. If you feel its hard to get, Just hang on. I would try to explain in brief later. . Now we define the that the parameters to be tuned is A . A = nn.Parameter(A); A . Parameter containing: tensor([-41289., 22000.], requires_grad=True) . def update(): Y_hat = X@A loss = mse(Y,Y_hat) if t %10 ==0: print(loss) # We are asking to print the loss every 10 iterations loss.backward() # Calculates the gradient value with torch.no_grad(): # Saying that we dont want to back propogate thereby reducing the memory and time footprint. A.sub_(lr * A.grad) # Now we are making a tiny little step towards the minimum value. where lr is the learning rate A.grad.zero_() # We are resetting the gradient value to zero . Here, we are calculating the gradient value using loss.backward() method. . Then we say we are done with the calculation using torch.backward() using torch.no_grad(). This is more of a Pytorch thing, so that it can reduce the memory and processing power allocated the backward propogation task (calculating the gradients). . Then perform the following calculation: . begin{equation} A = A- alpha*dA end{equation}where $ alpha$ is the learning rate and $dA$ is the change in A value as we move the value a bit(say 0.01). When $dA$ provides the slope, learning rate ($ alpha$) tells us how fast we must move. . So, we are making a tiny step towards minimising the error and optimising the $A$ value. . A graphical representation of this could give us more clarity. We can discuss about it soon. . We are running the update() function 100 times so that we can move towards the actual value. . lr = 1e-1 for t in range(100): update() . tensor(1.4036e+10, grad_fn=&lt;MeanBackward1&gt;) tensor(3.2934e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(1.4071e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(9.9072e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.9762e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7679e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7213e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7108e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7085e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7080e+08, grad_fn=&lt;MeanBackward1&gt;) . Now, we could see that the Loss value getting decreased gradually. . Lets plot the values ourselves to find out if we have converged to our solution . plt.scatter(X[:,0],Y) plt.scatter(X[:,0],X@A) plt.xlabel(&#39;Centuries&#39;) plt.ylabel(&#39;Auction Price&#39;) . Text(0,0.5,&#39;Auction Price&#39;) . Yes, it seems it indeed converge to the solution. We can see that, we have taught the machine to learn the value of A, by itself. We can check if the value of A is what we used to generate the value of the dataset . A . Parameter containing: tensor([135078.0156, 53438.6484], requires_grad=True) . It is indeed close. We can find that the value of $a$ is almost the same value as what we used. $b$ being a little different though. But this could have been caused due to the additional random value that we introduced while generating the dataset. . So, now if any new input or sets of inputs, on number of centuries (X) is given, it can find the auction price (Y) for it. . Understanding using Graphs . Now, lets look at the values step by step to understand what was happening . Impact of Iterations in prediction accuracy . from matplotlib import animation, rc rc(&#39;animation&#39;,html=&#39;jshtml&#39;) . A =nn.Parameter(tensor(-1.,1)) fig = plt.figure() plt.scatter(X[:,0],Y,c=&#39;orange&#39;) plt.xlabel(&#39;Centuries&#39;) plt.ylabel(&#39;Auction Price&#39;) line, = plt.plot(X[:,0],X@A) plt.close() def animate(i): update() line.set_ydata(X@A) return line, animation.FuncAnimation(fig,animate,np.arange(0,100),interval=20) . &lt;/input&gt; Once &lt;/input&gt; Loop &lt;/input&gt; Reflect &lt;/input&gt; Now we can clearly see that the arbitrary line that we just drew converged to the exact line that intended line. So, On each iteration of update() we see that the line turns one step closer to where we would intend the line to go towards. All we did was try to minimize the loss function and redraw the line by updating the $A$ value. This moves the line towards where we would like it to move towards. After n iterations (100 here), we see that it the loss becomes minimal and couldn&#39;t find a line that is can have a lower loss than the line above. . Loss Function Vs Value . The Loss Function shows how wrong the prediction is versus actual. Lets see the error rate along different values of $a$, considering $b$ as constant. . b=22000. . mse_list=np.empty(shape=(2,0)) for a in range(124112,154112): A=tensor([a,b]) Y_test=X@A mse_val=mse(Y,Y_test) mse_np=np.array([a,mse_val]).reshape([2,1]) mse_list=np.append(mse_list,mse_np,axis=1) . fig = plt.figure() plt.plot(mse_list[0,:],mse_list[1,:]) plt.xlabel(&#39;a&#39;) plt.ylabel(&#39;Error&#39;) . Text(0,0.5,&#39;Error&#39;) . Now, we can see the value of Error. We can also relate that the value gets towards 0 as we reach an optimum value for $a$. . Loss Function Vs Iterations . Now, lets look how the value of loss goes for this iterations of gradient descent. . def update(): Y_hat = X@A loss = mse(Y,Y_hat) if t %10 ==0: print(loss) loss.backward() with torch.no_grad(): A.sub_(lr * A.grad) A.grad.zero_() return loss . A =nn.Parameter(tensor(-1000.,2)) a_values=np.empty(shape=(2,0)) losses=np.empty(shape=(2,0)) for t in range(100): loss=update() loss_np=np.array([t,loss]).reshape([2,1]) losses=np.append(losses,loss_np,axis=1) a_np=np.array([A[0],mse(Y,X@A)]).reshape([2,1]) a_values=np.append(a_values,loss_np,axis=1) . tensor(1.1503e+10, grad_fn=&lt;MeanBackward1&gt;) tensor(2.2840e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(1.1763e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(9.3902e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.8605e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7420e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7155e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7095e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7082e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7079e+08, grad_fn=&lt;MeanBackward1&gt;) . fig = plt.figure() plt.plot(losses[0,:99],losses[1,:99]) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Error&#39;) . Text(0,0.5,&#39;Error&#39;) . Now, We could see that the Cost or Error reduces as the number of iterations increase. After a point, we no longer see a decrease in error rate. This error is due to the anonymity. . Note The post is mainly inspired from the Fast AI course. .",
            "url": "https://nareshr8.github.io/ml_posts/pytorch/fast-ai/2019/01/16/linear-regression-coding.html",
            "relUrl": "/pytorch/fast-ai/2019/01/16/linear-regression-coding.html",
            "date": " • Jan 16, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Understanding of Fast AI - Course V3 (Part 1 - Lesson 1)",
            "content": "This is the notes of first lesson of the list of lessons in the Part 1 of Fast AI V3 course. Though I went though few lessons on Fast AI course of last year, I was sure to do this course. This is the first course that comes after the first version release of Fast AI library itself. Last year it was in Beta Version (think 0.7 when I took it online) and it is now into version 1.0.14. . What’s your pet . In this lesson we will build our first image classifier from scratch, and see if we can achieve world-class results. Let’s dive in! . The lines in jupyter notebook that starts with ‘%’ are called Line Magics. These are not instructions for Python to execute, but to Jupyter notebook. . They ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook. . The reload_ext autoreload reloads modules automatically before entering the execution of code typed at the IPython prompt. . The next line autoreload 2 imports all modules before executing the typed code. . The documentation on autoreload is available here . The next line is to plot the graphs inside the jupyter notebook. We use matplotlib inline. . %reload_ext autoreload %autoreload 2 %matplotlib inline . We import all the necessary packages. We are going to work with the fastai V1 library which sits on top of Pytorch 1.0. The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models. . from fastai import * from fastai.vision import * . Looking at the data . We are going to use the Oxford-IIIT Pet Dataset by O. M. Parkhi et al., 2012 which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate “Image”, “Head”, and “Body” models for the pet photos. Let’s see how accurate we can be using deep learning! . We are going to use the untar_data function to which we must pass a URL as an argument and which will download and extract the data. . help(untar_data) . Help on function untar_data in module fastai.datasets: untar_data(url: str, fname: Union[pathlib.Path, str] = None, dest: Union[pathlib.Path, str] = None, data=True) Download `url` if doesn&#39;t exist to `fname` and un-tgz to folder `dest` . The untar_data is great idea for downloading the URL provided and download and untar. . path = untar_data(URLs.PETS); path . PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet&#39;) . In Python, we can extend and add new functionalities to the existing python modules. I found this link useful for creating one. . path.ls() . [PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/annotations&#39;)] . The pathlib thats part of Python 3 has the notation / which is useful to navigate into the directory as in the actual directory. We use to create Path variables with the new location. . path_anno = path/&#39;annotations&#39; path_img = path/&#39;images&#39; . Lets check all the images in the image directory . fnames = get_image_files(path_img) fnames[:5] . [PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/leonberger_34.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/pug_203.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/Siamese_203.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/scottish_terrier_98.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/beagle_76.jpg&#39;)] . np.random.seed(2) pat = r&#39;/([^/]+)_ d+.jpg$&#39; . The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like. . The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, ImageDataBunch.from_name_re gets the labels from the filenames using a regular expression. A detailed explaination found on an interesting read about the same lines of code in this tutorial is here. . Loading Images: . ImageDataBunch is used to do classification based on images. We use the method from_name_re to represent that the name of the classification is to be got from the name of the file using a regular expression. It takes the following parameters: . path_img the path of the images directory | fnames the list of files in that directory | pat the regex pattern that is used to extract label from the file name | ds_tfms the transformations that are needed for the image. This includes centering, cropping and zooming of the images. | size the size to which the image is to be resized. This is usually a square image. This is done because of the limitation in the GPU that the GPU performs faster only when it has to do similar computations (such as matrix multiplication, addition and so on) on all the images. | . Data Normalisation: . This is done to ensure that the images are easy to do mathematical calculations that we are looking for after that. This includes changing the range of values of RGB from 0-255 to -1 to 1.This is because we have 3 color channels namely Red, Green and Blue. For the pixel values we might have some color channels that varies slightly and some that doesn’t. So, we need to normalise the images with mean as 0 and standard deviation as 1. . One another thing to note is that we are using the residual network, which is pretrained. So, we must use the same normalisation that the residual network is using in order to use the best of the pretrained model. . data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224) data.normalize(imagenet_stats) . &lt;fastai.vision.data.ImageDataBunch at 0x7f21d8961eb8&gt; . Now, we can look into the image samples along with the classification name to check if everything that we have done thus far is doing great. . Its important to check this as we may understand some images might have some kind of issue over the other, like rotated in odd ways, just text on it, 2 different categories of classificatiers on it and so on. . doc(ImageDataBunch) . data.show_batch(rows=3, figsize=(7,6)) . . We use the data.classes to indicate the total number of distinct labels that were found. In our case, since wwe have extracted the labels from the regular expression, it indicates the number of distinct labels that were extracted from the regular expression. . data.c gives the total number of classifications that were found in the dataset . print(data.classes) len(data.classes),data.c . [&#39;leonberger&#39;, &#39;pug&#39;, &#39;Siamese&#39;, &#39;scottish_terrier&#39;, &#39;beagle&#39;, &#39;Birman&#39;, &#39;Abyssinian&#39;, &#39;great_pyrenees&#39;, &#39;chihuahua&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;yorkshire_terrier&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;pomeranian&#39;, &#39;newfoundland&#39;, &#39;Bombay&#39;, &#39;shiba_inu&#39;, &#39;german_shorthaired&#39;, &#39;Bengal&#39;, &#39;samoyed&#39;, &#39;boxer&#39;, &#39;wheaten_terrier&#39;, &#39;miniature_pinscher&#39;, &#39;english_cocker_spaniel&#39;, &#39;Maine_Coon&#39;, &#39;Sphynx&#39;, &#39;British_Shorthair&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;keeshond&#39;, &#39;saint_bernard&#39;, &#39;american_pit_bull_terrier&#39;, &#39;Russian_Blue&#39;, &#39;american_bulldog&#39;, &#39;english_setter&#39;, &#39;Egyptian_Mau&#39;, &#39;basset_hound&#39;] (37, 37) . Training: resnet34 . Now we will start training our model. We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. Since we are using Residual Network with 34 hidden units, all we have to do is to add a layer at the end on the residual network to transform the dimension of the residual network to the required output. In our case, it is to the 37 possible outputs. . We will train for 4 epochs (4 cycles through all our data). . We create a learner object that takes the data, network and the metrics . The metrics is just used to print out how the training is performing. We choose to print out the error_rate. . learn = create_cnn(data, models.resnet34, metrics=error_rate) . Downloading: &quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&quot; to /home/nbuser/.torch/models/resnet34-333f7ec4.pth 100%|██████████| 87306240/87306240 [00:02&lt;00:00, 29535503.58it/s] . learn.fit_one_cycle(4) . Total time: 03:19 epoch train_loss valid_loss error_rate 1 1.156555 0.291909 0.091151 (00:53) 2 0.505356 0.249506 0.077179 (00:48) 3 0.312138 0.212065 0.074518 (00:49) 4 0.240234 0.198288 0.069195 (00:48) . learn.save(&#39;stage-1&#39;) . Results . Let’s see what results we have got. . We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. . Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour. . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_top_losses(9, figsize=(15,11)) . . doc(interp.plot_top_losses) . interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . . interp.most_confused(min_val=2) . [(&#39;Ragdoll&#39;, &#39;Birman&#39;, 6), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 5), (&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 5), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 5), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 4), (&#39;British_Shorthair&#39;, &#39;Russian_Blue&#39;, 4), (&#39;Bengal&#39;, &#39;Egyptian_Mau&#39;, 3), (&#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, 3), (&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 3), (&#39;american_pit_bull_terrier&#39;, &#39;american_bulldog&#39;, 3), (&#39;american_bulldog&#39;, &#39;staffordshire_bull_terrier&#39;, 3)] . Unfreezing, fine-tuning, and learning rates . Since our model is working as we expect it to, we will unfreeze our model and train some more. . learn.unfreeze() . learn.fit_one_cycle(1) . Total time: 01:07 epoch train_loss valid_loss error_rate 1 1.070711 0.524961 0.168995 (01:07) . Since the Model underperformed while training after unfreeze, we would like to move to our previous best model that we have saved stage-1. We will finetune to improve from here. . learn.load(&#39;stage-1&#39;) . We use the lr_find method to find the optimum learning rate. Learning Rate is an important hyper-parameter to look for. We traditionally use $ alpha$ to denote this parameter. If the Learning rate is too slow, we take more time to reach the most accurate result. If it is too high, we might not even end up reaching the accurate result. Learning Rate Finder was idea of automatically getting the magic number (which is near perfect), to get this optimum learning rate. This was introducted in last year’s Fast AI course and continues to be useful. . learn.lr_find() . LR Finder complete, type {learner_name}.recorder.plot() to see the graph. . After we run the finder, we plot the graph between loss and learning rate. We see a graph and typically choose a higher learning rate for which the loss is minimal. The higher learning rate makes sure that the machine ends up learning faster. . learn.recorder.plot() . . We see around $1e^{-4}$ mark, we have a optimum learning rate. Now that we know the optimum learning rate. . Considering the fact that we are using a pretrained model of resnet-34, we know for sure that our previous layers of this neural network would learn to detect the edges and the later layers would learn complicated shapes such as the dogs and cats itself. We don’t want to ruin out the earlier layers which presumably does a good job of detecting the edges. But would like to improve the model in narrowing down classifying the image of dogs and cats to our needs, which is done in the later layers. . So, we will set a lower learning rate for earlier layers and higher one for the last layers. . The slice is used to provide the learning rate wherein, we just provide the range of learning rates (its min and max). The learning rate is set gradually higher as we move from the earlier layer to the latest layers. . learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4)) . Total time: 02:14 epoch train_loss valid_loss error_rate 1 0.199850 0.185868 0.064538 (01:07) 2 0.194062 0.182656 0.065203 (01:07) . That’s a pretty accurate model! . Training: resnet50 . Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. Later in the course you can learn the details in the resnet paper). . Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let’s see if we can achieve a higher performance here. . data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=299, bs=48) data.normalize(imagenet_stats) . &lt;fastai.vision.data.ImageDataBunch at 0x7f21d89b3860&gt; . learn = create_cnn(data, models.resnet50, metrics=error_rate) . learn.fit_one_cycle(5) . Total time: 16:40 epoch train_loss valid_loss error_rate 1 0.646226 0.257891 0.085036 (03:53) 2 0.348598 0.244830 0.082399 (03:11) 3 0.236005 0.192446 0.061964 (03:11) 4 0.149788 0.147233 0.044825 (03:11) 5 0.100550 0.138161 0.048121 (03:11) . learn.save(&#39;stage-1-50&#39;) . It’s astonishing that it’s possible to recognize pet breeds so accurately! Let’s see if full fine-tuning helps: . learn.unfreeze() learn.fit_one_cycle(1, max_lr=slice(1e-6,1e-4)) . Total time: 04:27 epoch train_loss valid_loss error_rate 1 0.088951 0.151667 0.050758 (04:27) . In this case it doesn’t, so let’s go back to our previous model. . learn.load(&#39;stage-1-50&#39;) . We now load the previous best model and would like to improve upon that model. . interp = ClassificationInterpretation.from_learner(learn) . interp.most_confused(min_val=2) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 6), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 5), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 5), (&#39;Ragdoll&#39;, &#39;Persian&#39;, 4), (&#39;staffordshire_bull_terrier&#39;, &#39;american_bulldog&#39;, 4), (&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 3)] . learn.lr_find() . LR Finder complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot() . . learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4)) . Total time: 08:26 epoch train_loss valid_loss error_rate 1 0.116429 0.138112 0.049440 (04:17) 2 0.084291 0.129927 0.044166 (04:08) . learn.save(&#39;stage-2-50&#39;) . Save the model as it seems a little more accurate . Other data formats . path = untar_data(URLs.MNIST_SAMPLE); path . PosixPath(&#39;/home/jhoward/.fastai/data/mnist_sample&#39;) . tfms = get_transforms(do_flip=False) data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26) . data.show_batch(rows=3, figsize=(5,5)) . . learn = ConvLearner(data, models.resnet18, metrics=accuracy) learn.fit(2) . VBox(children=(HBox(children=(IntProgress(value=0, max=2), HTML(value=&#39;0.00% [0/2 00:00&lt;00:00]&#39;))), HTML(value… Total time: 00:11 epoch train loss valid loss accuracy 1 0.108823 0.025363 0.991168 (00:05) 2 0.061547 0.020443 0.994112 (00:05) . df = pd.read_csv(path/&#39;labels.csv&#39;) df.head() . name label . 0 train/3/7463.png | 0 | . 1 train/3/21102.png | 0 | . 2 train/3/31559.png | 0 | . 3 train/3/46882.png | 0 | . 4 train/3/26209.png | 0 | . data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28) . data.show_batch(rows=3, figsize=(5,5)) data.classes . [0, 1] . . data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24) data.classes . [0, 1] . fn_paths = [path/name for name in df[&#39;name&#39;]]; fn_paths[:2] . [PosixPath(&#39;/home/jhoward/.fastai/data/mnist_sample/train/3/7463.png&#39;), PosixPath(&#39;/home/jhoward/.fastai/data/mnist_sample/train/3/21102.png&#39;)] . pat = r&quot;/( d)/ d+ .png$&quot; data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24) data.classes . [&#39;3&#39;, &#39;7&#39;] . data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24, label_func = lambda x: &#39;3&#39; if &#39;/3/&#39; in str(x) else &#39;7&#39;) data.classes . [&#39;3&#39;, &#39;7&#39;] . labels = [(&#39;3&#39; if &#39;/3/&#39; in str(x) else &#39;7&#39;) for x in fn_paths] labels[:5] . [&#39;3&#39;, &#39;3&#39;, &#39;3&#39;, &#39;3&#39;, &#39;3&#39;] . data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24) data.classes . [&#39;3&#39;, &#39;7&#39;] . .",
            "url": "https://nareshr8.github.io/ml_posts/fast-ai/2018/10/31/fastai-p1-l1.html",
            "relUrl": "/fast-ai/2018/10/31/fastai-p1-l1.html",
            "date": " • Oct 31, 2018"
        }
        
    
  
    
        ,"post5": {
            "title": "Getting Started in Machine Learning - Basics",
            "content": "Machine Learning - Basics . What is Machine Learning? . There are many perspectives on what machine learning is. But a well accepted definition is : . A Field of study that gives computers the ability to learn without being explicitly programmed - Arthur Samuel (1959) . A slightly complicated to understand but accurate one would be . A computer program is said to learn from experience E with respect to task T and some performance measure P, if its performance on T, as measured by P, improves with experience E - Tom Mitchell (1998) . Types of Machine Learning . There are most of the machine learning concepts some in one among any of the carders: . Supervised Learning - | Unsupervised Learning | Reinforcement Learning | Recommender Systems | . Supervised Learning . Supervised learning is a learning technique used when we have a sample data with a given set of inputs and its corresponding ‘expected value’, we expect the computer to predict the ‘expected value’ when a new set of inputs is given to it. . Example: . We would take the most common example of Housing Price prediction. We would be given a list of ‘features’ of the house, area of the house and the cost of the house in that area, Our goal would be to predict the price of the house for an unseen house for which we know the height and width. . . Now, we mark the area of the house (X-axis) along with its price (Y-axis) plotted in a graph as a (X) mark. . Now, what we do by the process called training, is to draw a function f(x) which maps x to y. In essence, y=f(x) is the function we will try to come up with so that for any new value of x, we can find the value of y by a process called predict. Here the list of features ‘x’ is called independent variance and the expected output ‘y’ is the dependent variable. Here the expected value ‘y’ is called independent variable. . If the independent variable is continuous valued, the type of supervised machine learning is called regression. The above one is a good example of regression based supervised machine learning. . The contradictory example is to predict the type of cancer, Malignant or Benign based on the size of the tumor. Here, the size of the tumor is the independent variable whereas, Cancer type is the dependent variable and can take one of the two values (Malignant or Benign) only. Hence this type of machine learning is called classification. . . Unsupervised Learning . Unsupervised Learning is another category of Machine Learning where we would not be given the right set of answers. Instead, we give a set of data and ask the system, can you find a pattern in this data. . Example . One example of the unsupervised learning might be customer segmentation. We give give the system the customer segments and look for the system to segment out the customers without explicitly saying which segment any customer belongs to. . . Reinforcement Learning . Reinforcement Learning is a category of machine learning where the machine learns to perform a Task in an environment so as to maximize the reward over the long term. . A famous example might be the Alpha Go, developed by Google which bet the professional Go players. . Recommendation Systems . Recommendation systems are a class of machine learning which would predict the rating or preference of the user to a given item. . Example . Common examples are recommended products that are available in almost all online shopping sites such as Amazon, Flipkart, Ebay. . . Disclaimer: Most part of the contents in this blog are from the Machine Learning course by Andrew Ng. .",
            "url": "https://nareshr8.github.io/ml_posts/ml/2018/10/02/ml-basics.html",
            "relUrl": "/ml/2018/10/02/ml-basics.html",
            "date": " • Oct 2, 2018"
        }
        
    
  
    
        ,"post6": {
            "title": "Object Localization",
            "content": "Today, we work for a particular problem statement, image localization. Image Localization basically means that we will look into a given image and not just say whether a stamp is available in the picture or not but also where it is, if available. . The problem statement is easy for a kid to perform, but was impossible few years back for even super computers to accurately say like a kid. Thanks to the advancement in computational power and deep learning literature, we are now able to make computers do this task. . We are using a python library “Fast AI” to perform this task. If you are little unfamiliar with Fast AI, checkout fast.ai. It’s the easiest way to get your hands on machine learning and do some exciting tasks straight up. . Problem Statement . Your company sends the manufactured products out of the garage only when they have been tested. The tested products have a seal like this: . You are a data scientist or a machine learning engineer who is trying to use computers to check if the product is tested or not instead of humans, as it speeds up the disposal of the products. . Why Data Augmentation . Most of the time the company doesnt have enough images to perform machine learning tasks on. This is also a typical case in your company. Your company can only give you the image of the stamp. You try to collect the data yourself. However, You need a camera to take pictures on the product’s cover art so that you can collect thousands of images. Your company asks for a proof that your product will work before it can install the camera on the product disposal area. This lands you in a place where you have to get the product working before the main component of the product (data) is available. So, we have to augment the data. . Data Augmentation . We plan to augment whatever data is needed. We crawl through the web and pull images of various products and superimpose this stamp over them to create data that we are looking for. We thereby have a set of images that is available with/without stamp and the location if its available. . Image Crawling . For image crawling in python we have a library ‘icrawler’, which I have used and seem to pull images from different data sources like Google, Bing, Baidu. We can use the crawler to pull the images from internet. The working crawler notebook is available here. . Why crawl thousands of images? . If we have only 10 images in training set as background and superimpose the stamp in various locations, there is a possibility of overfitting, meaning that the machine will try to remove the 10 possible backgrounds and check if any of the remaining pixel is having the value, instead of looking at the stamp. Which will not be the ideal scenario for the actual images. So, even if its augmentation, the more data the better. . Image Super-Imposition . I have used PIL library to super impose the images. The image preprocessing is done in this notebook. . Training the Model . For training the model, we train the model using the notebook that is available here. . Other variants . Here we used only one stamp and we are checking if that particular stamp is available. Instead, we can generate data with all the variety of labels with all variants of stamps as well and pretty much follow the same procedure. We would be able to train the model to check for any of the stamps is available and the location. The other alternative is Person Tagging. Similar to stamps, we can give a set of images with people faces and ask the machine to tag the person’s face on the image. . Improvisation . As part of improvising, we can . tag multiple items on the same image. | identify each class of the tagged image. | Use this process as part of a end to end solution. For example, if we want to know the price of a product, we naturally find the location where the price is listed. After localizing the place where the price is listed, we try to read the price. We can do the same with machines to understand the price of the product. | Use to develop Optical Character Recognition. We tag each character and a classifier that classifies between A-Z and Numbers. The characters with lesser space between them forms a word. | . I may try some of these myself and post if something really cool works out. . Post your comments and let know your views on this. .",
            "url": "https://nareshr8.github.io/ml_posts/object-localisation/cnn/resnet/fast-ai/fastai/2018/08/15/object-localisation.html",
            "relUrl": "/object-localisation/cnn/resnet/fast-ai/fastai/2018/08/15/object-localisation.html",
            "date": " • Aug 15, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Preprocessing Structured Data For Machine Learning - I",
            "content": "Preprocessing the data is the first part of any machine learning project that we take up. This blog post, being my first ever, will start to discuss about preprocessing of data in python. I used Jupyter Notebook/lab as the IDE of preference along side Python 3. . Why Data Preprocessing? . Data Preprocessing ensures that the data is available in the right format for machine learning to be performed. . The golden rule for any machine learning project is more the data, the better. So, We might collect data from various sources. All data may not be good straight away for starting machine learning. We may run into one or many of these problems, most of the time: . Missing Data - Some columns might be missing in some rows. | Incorrect Data - Some data might have been wrong due to manual entry or inconsistent data source | Feature Scaling - Algorithms such as KNN, SVM prefer uniform distribution among the dataset as it uses distance or similarities between datasets. | . Now that the need of preprocessing is felt, we can start to preprocess the data. Here, the theme is to get started building a notebook with functions that are commonly used in preprocessing structured data that can be used for any structured data. . Prerequisites . We are using the Following libraries for making our process easier. . Pandas . To install run this on your python console : . !pip install pandas . ​ . | Numpy . To install run this on your python console : . !pip install numpy . | . A simple google might solve the problem if you have any trouble installing these packages. . Preprocessing Steps . First we have to import our necessary packages . import numpy as np import pandas as pd . Now collect the data into Data Frame. We can collect data from different sources into a Data Frame. Since we are creating the just utilities that take in data frame as input, we can skip this step. . Getting list of Missing Values . We may have to deal with the fact that there will be missing values in one or more columns in our dataset. First we may have to take a look on how many columns have missing values. So we can have a helper method do that for us. . def get_missing_valued_columns_list(dataset): return dataset.columns[dataset,isnull().any()] . This gives the list of columns which has missing values. . Now that we see the list of columns which has missing values, we might be interested in knowing how many values among those are missing. If there are columns that misses more than, say 80% of data, we might chose to ignore that column. . Getting List of Columns with missing count . Now we look to get the data that shows the list of columns with the count of data that is missing in those column . def get_missing_valued_column_details(dataset): sum_of_missing_values = dataset.isnull().sum(axis=0) return sum_of_missing_values[sum_of_missing_values &gt; 0] . Here, we first summed up the count of data that has values as null (along the vertical axis), then we filtered out non empty columns. . Get labels which doesn’t have enough data . If our label doesn’t have enough data, we cannot make useful predictions out of the data. Thus, can remove columns that doesn’t have enough data in them. . def get_low_variance_columns(dataset): from sklearn.feature_selection import VarianceThreshold columns = dataset.iloc[:,:-1].columns selector = VarianceThreshold(.8 * (1-.8)) selector.fit(dataset.iloc[:,:-1]) labels = [columns[x] for x in selector.get_support(indices=True) if x] return labels . Here we are selecting all columns except the last one, as last column is usually the output or prediction column. Then we use VarianceThreshold function thats part of scikit learn library to specify the threshold (.8) to retrieve the list of columns. . I am planning to add other commonly used functions in my next blog post. Comment on your views. .",
            "url": "https://nareshr8.github.io/ml_posts/preprocessing/machine-learning/2018/05/21/data-pre-processing-i.html",
            "relUrl": "/preprocessing/machine-learning/2018/05/21/data-pre-processing-i.html",
            "date": " • May 21, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Application Developer at Tata Consultancy Services . Developing Machine Learning, Java and Native Android Applications. . Have worked under various clients such as Walmart, Home Depot, Century Link, Qantas, Nielsen, Vodafone Hutchison Australia since Nov 2011. . Education . M.Tech in Software Systems - BITS Pilani (2013-2015) . B.Tech in Information Technology - Anna University (2007-2011) . Hobbies . Playing Cricket, Listening Music, watching YouTube and now starting to blog . Programming . I ♥ programming. I love to code and develop interesting stuff myself. . I have developed passion towards machine learning and started working for more than 2 years. Have been certified for Deep Learning Specialisation and Advanced Machine Learning with TensorFlow on Google Cloud Platform in Coursera. . Also, I am also certified in Android Application Development .I have developed few android apps myself and put on Play Store .",
          "url": "https://nareshr8.github.io/ml_posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}