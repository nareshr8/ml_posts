{
  
    
        "post0": {
            "title": "Classical Dances of India",
            "content": "There are various classical dances that are available all throughout in India. These classical dances are culture specific. For example, Tamil Nadu follows a classical dance called Bharathanaatiyam where as its nearby state, Kerala has a classical dance of Kathakali. . Being not much knowledgable in any of the classical dances, I was triggered by the idea of Jeremy Howard who was suggesting that we might be able to solve deep learning problems even if we ourselves arent the domain experts. So, I decided to try myself out if I could build a model that can perform well in classifying the classical dance names. . I have already done the same. But this time on the latest version of FastAi (V2). If you have checkd out the previous blog post on this, You may consider this as just a version updated blog. . Steps . To build a classifier, we have to build a classifier on our own that can be run end to end, we need to follow the below steps. . Download the dataset | Do preprocessing of data, if any | Create a model to detect the classical dance | Deploy it using a single GUI | . Downloading the Dataset . To download a dataset, we need some Image Search API. Bing is one such API which provides better image downloading capabilities. But we may have to register to Bing Image Search to get the access key to use the API. . from azure.cognitiveservices.search.imagesearch import ImageSearchClient as api from msrest.authentication import CognitiveServicesCredentials as auth from fastai2.vision.all import * from fastai2.vision.widgets import * def search_images_bing(key, term, min_sz=128,count=150): client = api(&#39;https://api.cognitive.microsoft.com&#39;, auth(key)) return L(client.images.search(query=term, count=count, min_height=min_sz, min_width=min_sz).value) . after signing up, add the key in the below cell and list the classes that you want to classify . key=&#39;xxx&#39; classes = [&#39;Bharathanatyam&#39;,&#39;Kathakali&#39;,&#39;Kathak&#39;,&#39;jagoi dance&#39;] . Select a path where you would love to do image classification . path=Path(&#39;./data/image-classification&#39;) . Download images for each class and put it in a different folder where the folder name specifies the class name . path.mkdir(exist_ok=True) for o in classes: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, o) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . After downloading images, verify if the files are generated as appropriate . fns = get_image_files(path) fns . (#591) [Path(&#39;data/image-classification/Bharathanatyam/00000080.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000001.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000125.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000062.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000124.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000015.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000144.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000033.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000140.jpg&#39;),Path(&#39;data/image-classification/Bharathanatyam/00000071.jpg&#39;)...] . Now that we have downloaded the images. Our next step is to train the model to learn identifying classical dance images. . Look into the images . After we downloaded the images, we need to check if the downloaded images have some corrupt images as well. These images will not open for some reason and hence break when we try to open. We dont want to train our network with these images. . Fast AI comes with a method to verify these images using verify_images method . failed = verify_images(fns) failed . (#2) [Path(&#39;data/image-classification/Kathak/00000069.jpg&#39;),Path(&#39;data/image-classification/Kathak/00000117.jpg&#39;)] . Delete the corrupted images by using unlink method in Path . failed.map(Path.unlink) . (#2) [None,None] . Creating DataBlock . Creating a datablock is the next step. DataBlock is the place where we specify essentially everything about the data that has to be done before giving it to the model. . classical_dances_db = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128)) . In the above code: . blocks is used to specify the way to generate Dependent and independent variables | get_items provides a way to get each item from the path. In this case, get individual image paths as files | splitter provides a way to split validation and training data. We have made sure the same set of images are used evertime we randomly divide the dataset into training and test set. | get_y a method to get Y value out of individual path value. Since getting Y from the path is little tricky i.e looking at the parent folder name, we specify the way to get y | item_tfms specifies transformations that are to be doneat item level. Such as resizing to 128*128 image (usually done through center cropping) | . There are some transforms like moving to GPU, Normalisation that are done implicitly. . Now we specified the shell of the data, we now load the data to be used for training using dataloaders. The dataloaders convinently converts all the data that are provided and do all the transformations to be done so that the data can be fed into the model for training. . dls = classical_dances_db.dataloaders(path) . Now that we have transformed the data, we might look at the data if it is transformed right. . dls.show_batch(max_n=4, nrows=1) . Now after some analysis we find that these images like ok. So, lets start by looking into training the machine . Training the model . Train the model with ResNet-18 and finetune 4 epochs . As we are all set for training the machine, we can continue to train the machine learn the type of dance from image . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.740276 | 2.458594 | 0.522727 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.900825 | 0.744960 | 0.244318 | 00:04 | . 1 | 0.693868 | 0.367972 | 0.102273 | 00:04 | . 2 | 0.556724 | 0.302496 | 0.102273 | 00:04 | . 3 | 0.457315 | 0.283369 | 0.096591 | 00:04 | . Here we created a cnn_learner which takes: . dls - The data to be loaded | arch - Pretrained Resnet Model (resnet18) so that we can ran faster | metrics - The metrics to be displayed after training | . Understanding the model . we will analyse the model so that we can get enough information out of it . Confusion Matrix is a matrix which tabulates the Actual Vs Predicted output. If both are same, then out machine did great job in identifying the image. Lets see how our model preformed . Look at the confusion matrix to see which one goes wrong mostly . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . The confusion is mostly around Kathak. The model gets wrong on Kathak often with Bharathanatyam and jagoi dance. . Look at the most confused images . interp.plot_top_losses(4, nrows=2) . Cleanup . There can be cases where some images are messy. Like, The search result which has a dancer who is famous in, say Bharathanatyam performing Kathak. These images can confuse the model in learning. Lets look if thats the case . This is also a place where we have to probably consult people who are data experts. They might help us to know if the image we call as Kathak were actually of Kathak Dance per say. . FastAI comes with this very cool widget which is very useful in these cases . cleaner = ImageClassifierCleaner(learn) cleaner . The output of the previous one is an interactive widget which allows you to select images that were wrongly classified or images that are to be deleted. . Some common images that had to be removed includes: . Photos of probably famous dances that were taken during some interview or award ceremony | Agenda of the dance festival | The ornaments and dresses thats associated with that dance | The image of artist while they do the make over or dressing while preparing for the dance. | . We had Images one of each type. Hence we do the appropriate using change and delete methods. . cleaner.change() cleaner.delete() . (#1) [7] . Rerunning the model after cleaning the data . Since the mode is rerun, the model might have a better at understanding now since the confusing images are removed. So, We are essentially repeating things that we were doing earlier . classical_dances_db_2 = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = classical_dances_db_2.dataloaders(path) . dls.show_batch(max_n=4, nrows=1) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.949091 | 1.575333 | 0.369318 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.945574 | 0.684837 | 0.238636 | 00:04 | . 1 | 0.711181 | 0.396144 | 0.119318 | 00:04 | . 2 | 0.528489 | 0.341853 | 0.090909 | 00:04 | . 3 | 0.423833 | 0.317606 | 0.102273 | 00:04 | . learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 0.113944 | 0.557393 | 0.096591 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.058908 | 0.532337 | 0.113636 | 00:04 | . 1 | 0.060650 | 0.552075 | 0.125000 | 00:04 | . 2 | 0.040418 | 0.566424 | 0.119318 | 00:04 | . 3 | 0.031581 | 0.566204 | 0.102273 | 00:04 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Add Augumentations . We do Data Augumentations, as this is a very basic work, we try to have default augumentations in aug_tfms. and try the process one more time . classical_dances_db_3 = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128), batch_tfms=aug_transforms()) . dls = classical_dances_db_3.dataloaders(path) . dls.show_batch(max_n=4, nrows=1) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.992708 | 1.989776 | 0.454545 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.966205 | 0.912835 | 0.278409 | 00:04 | . 1 | 0.805900 | 0.455987 | 0.147727 | 00:04 | . 2 | 0.669330 | 0.356659 | 0.113636 | 00:04 | . 3 | 0.567247 | 0.332295 | 0.107955 | 00:04 | . learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 0.317089 | 0.318796 | 0.107955 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.392914 | 0.311143 | 0.096591 | 00:04 | . 1 | 0.337256 | 0.339022 | 0.113636 | 00:04 | . 2 | 0.318557 | 0.297601 | 0.096591 | 00:04 | . 3 | 0.292514 | 0.286991 | 0.090909 | 00:04 | . interp.plot_top_losses(4, nrows=2) . Export the model . There is obviously more things that can be done here. But with some good defaults, we get around 91% accuracy. Since we are satisfied with them, lets export the model by calling learn.export() . learn.export() . Loading and Predicting the model . path = Path(&#39;/&#39;) path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . We can load the model using load_learner method. . learn_inf = load_learner(path/&#39;export.pkl&#39;) . Now we can load a random image. It can be outside our dataset. Just for ease I am choosing one from the dataset itself. . path=Path(&#39;./data/image-classification&#39;) . paths=(path/&#39;Bharathanatyam&#39;).ls() . paths[0] . Path(&#39;data/image-classification/Bharathanatyam/00000080.jpg&#39;) . Now we predict by calling the predict method passing in the path of the image . learn_inf.predict(str(paths[0])) . (&#39;Bharathanatyam&#39;, tensor(0), tensor([9.9981e-01, 8.2153e-05, 8.2317e-05, 2.8997e-05])) . As you can see, the file was in the Bharathanatyam folder and the prediction was also Bharathanatyam with the confidence of 99.98%. Hence we got the correct prediction .",
            "url": "https://nareshr8.github.io/ml_posts/image-classifer/fastai2/2020/04/02/Image-Classification.html",
            "relUrl": "/image-classifer/fastai2/2020/04/02/Image-Classification.html",
            "date": " • Apr 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "from fastai2.vision.widgets import * from fastai2.vision import * from fastai2.vision.all import * . btn_upload = widgets.FileUpload() . path = Path(&#39;/home/ml_team/ML/Fast-AI&#39;) path.ls(file_exts=&#39;.pkl&#39;) learn_inf = load_learner(path/&#39;export.pkl&#39;) . out_pl = widgets.Output() out_pl.clear_output() lbl_pred = widgets.Label() lbl_pred.value=&#39;&#39; . btn_run = widgets.Button(description=&#39;Classify&#39;); . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(256,256)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your Dance Image:&#39;), btn_upload, btn_run, out_pl, lbl_pred]) .",
            "url": "https://nareshr8.github.io/ml_posts/2020/04/01/Image-Classification-UI.html",
            "relUrl": "/2020/04/01/Image-Classification-UI.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding Linear Regression",
            "content": "The linear regression is the basic building block algorithm for most Machine learning algorithms. It is the simplest machine learning algorithm used for both regression and classification problems. It is used as a building block for most complicated architectures like Neural Network, Convolution Neural Network and Recurrent Neural Network. Hence having a basic understanding for it will give us a great overview of how complicated machine learning architectures are built upon. . Basics . For Machine Learning to happen, we need 3 groups of datasets, synonymous to a student learning maths: . Training Set: The data that we use to teach the machine (Text Book questions) | Test Set: The data that we use to test if it understood correctly (The Exam Paper) | Real World Set: The Real world example of using the learned skill. | . Lets take an example of the training set say centuries vs auction price. Its fairly obvious that if you have scored more centuries, you are a great player and hence you&#39;ll be bought for more price. Lets see how the machine learns this fact and how it could predict the auction price. . Centuries Auction Price . 2 | 200,000 | . 3 | 330,000 | . 5 | 450,000 | . 6 | 610,000 | . 7 | 750,000 | . Now we are making our student (Machine) understand the above table. But how could we do that. Lets see.. . Little Bit of Math &#128530; . Let&#39;s take Centuries scored as the x variable and Auction Price as y variable or the variable to be predicted. The price of a player will be based on the number of centuries, they scored. However, there is a fair say that there would be a base price even which doesn&#39;t depend on the number of centuries. . Lets say that we would like to fit this to a variable begin{equation} y=a*x+b end{equation} . Where the value of a and b are weights of centuries and base price that could affect the auction price and our task is to find this weight-age. . So for example i, we have the equation as follows: begin{equation} y_i=a*x_i+b end{equation} . So, if we have to calculate values for i examples of $x_i$ and $y_i$ variables, we need to do the computation in loop. . Now we can also consider $y_i=a*x^1_i+b*x^0_i$ . Let&#39;s call the complete vector of $x$ ($x^1$ and $x^0$) for all $i$ examples to be $X$ . Complete vector of $y$ (for $i$ examples) to be $Y$. Now the equation becomes like this. . begin{equation} Y=X.A end{equation}where begin{align*} A = begin{bmatrix} a b end{bmatrix} end{align*} . As we consider that the value b is multiplied by $ x^0 $ (which is 1). So, begin{align*} X = begin{bmatrix} x_0 x^0_0 x_1 x^0_1 vdots x_n x^0_n end{bmatrix} end{align*} . and begin{align*} Y = begin{bmatrix} y_0 y_1 vdots y_n end{bmatrix} end{align*} where the subscript represents the example number (1 through n) . The Shape of Y is $n*1$ and $X$ is $n*2$, where n represents the number of examples. . We can see a working demo of how this equation sums up here. . Finding the Coefficients . Now we know what is X and Y. But, we don&#39;t know what is A (i.e) a and b. We are going to use Stochastic Gradient Descent to identify them. . We can use Pytorch for finding this. . Since FastAI uses Pytorch and has some additional capabilities, we can import this one which internally imports Pytorch. So, lets import the library here. . %matplotlib inline %matplotlib nbagg from fastai.basics import * . Now that we have imported the library, lets create some random dataset. . Note: We could use the dataset in the table above, but it is very small amount of data as compared to what we need. So lets generate some random data . n=100 . Now for the 100 records, we are creating a matrix of 2 dimensions, n representing the number of records and 2 representing each example having 2 values. One value being dynamic $x^1$ and other being $x^0$ (i.e. 1). Now $x^1$ may have varied values, however $x^0$ will always be one. . So, we . create a 2D matrix of $100*2$ values of all ones | Replace the first column with values from a uniform distribution $[-1,1)$ where -1 is included and 1 is excluded | . We then verify if the value got updated properly. . X= torch.ones(n,2) X[:,0].uniform_(-1,1) X[:5] . tensor([[ 0.7498, 1.0000], [ 0.2281, 1.0000], [-0.5831, 1.0000], [ 0.4266, 1.0000], [-0.2114, 1.0000]]) . We then create a tensor with 2 values that could represent A. We arbitrarily set a random value. Lets take it as 134112 and 220. where $a=134112$ and $b=220$. This is done just to create a dataset with some values. But in real world scenario, $A$ is the value that we would have to find. . A=tensor([134112.,220]) . Now, we are going to do the matrix product of the values X and A to get our Y value. Note that we are adding a random value here so that we dont have . Y=X@A+((torch.rand(n)*100000)); . Lets see how it looks in a graph . plt.scatter(X[:,0], Y) . &lt;matplotlib.collections.PathCollection at 0x7fbc70166cc0&gt; . Now, our goal is to find the correct value of begin{align*} A = begin{bmatrix} a b end{bmatrix} end{align*} so that we can predict $Y$ for any new set of examples $X$. To do that we have to calculate something called error. . Error . Now lets look at what values we could get for a random value of A. A = $ begin{align*} begin{bmatrix}4112. 22000 end{bmatrix} end{align*}$ gives the value as below. . A=tensor([-41289.,22000]) plt.scatter(X[:,0], Y) plt.plot(X[:,0], X@A,c=&quot;orange&quot;) plt.xlabel(&#39;Centuries&#39;) plt.ylabel(&#39;Auction Price&#39;) . Text(0,0.5,&#39;Auction Price&#39;) . Now we can clearly see that The Orange color line is way to far from actually predicting the auction price. Now, we have to define far. This is where error comes in. We want to give a number to how far it is from the actual result. . Classical Error function . Now, just by looking into the data we would say the error is the difference between the actual value versus the predicted value. Well, That can be one of the error function. This can look like this: . $J(a,b) = sum_1^n(y- hat{y} )$ . where J denotes the cost function, $a$ denotes the parameters which fit in A. . However, In practice we are using a different cost function (Note that error function and cost function dentotes the same). This is because if we magnify the error function a little bit, we converge to the correct value more than stating the actual value, using gradient descent. (This is proven to work in machines as well..). So, we take the error function as Mean Squared Error . Mean Squared Error . Mean squared error basically takes the square value of the difference. So, The loss function changes to a little bit different. . $J(a,b) = sum_1^n(y- hat{y})^2$ . So, we can create the function J as follows: . def mse(y_hat,y): return ((y_hat-y)**2).mean() . Optimisation . Now that we have defined the error function, we are trying to define a way to optimize the value of A such that we reduce the error, intern predicting the value of $ hat{Y}$ (predicted value) closest the actual $Y$ (actual value). Gradient Descent is an algorithm which is used to reduce the value of a given function. The way it works is by taking the initial set of parameters $A$ and iteratively optimizing the value to minimize the function. . This iterative minimisation is achieved by taking individual steps in the negative direction of the gradient function. If you feel its hard to get, Just hang on. I would try to explain in brief later. . Now we define the that the parameters to be tuned is A . A = nn.Parameter(A); A . Parameter containing: tensor([-41289., 22000.], requires_grad=True) . def update(): Y_hat = X@A loss = mse(Y,Y_hat) if t %10 ==0: print(loss) # We are asking to print the loss every 10 iterations loss.backward() # Calculates the gradient value with torch.no_grad(): # Saying that we dont want to back propogate thereby reducing the memory and time footprint. A.sub_(lr * A.grad) # Now we are making a tiny little step towards the minimum value. where lr is the learning rate A.grad.zero_() # We are resetting the gradient value to zero . Here, we are calculating the gradient value using loss.backward() method. . Then we say we are done with the calculation using torch.backward() using torch.no_grad(). This is more of a Pytorch thing, so that it can reduce the memory and processing power allocated the backward propogation task (calculating the gradients). . Then perform the following calculation: . begin{equation} A = A- alpha*dA end{equation}where $ alpha$ is the learning rate and $dA$ is the change in A value as we move the value a bit(say 0.01). When $dA$ provides the slope, learning rate ($ alpha$) tells us how fast we must move. . So, we are making a tiny step towards minimising the error and optimising the $A$ value. . A graphical representation of this could give us more clarity. We can discuss about it soon. . We are running the update() function 100 times so that we can move towards the actual value. . lr = 1e-1 for t in range(100): update() . tensor(1.4036e+10, grad_fn=&lt;MeanBackward1&gt;) tensor(3.2934e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(1.4071e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(9.9072e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.9762e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7679e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7213e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7108e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7085e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7080e+08, grad_fn=&lt;MeanBackward1&gt;) . Now, we could see that the Loss value getting decreased gradually. . Lets plot the values ourselves to find out if we have converged to our solution . plt.scatter(X[:,0],Y) plt.scatter(X[:,0],X@A) plt.xlabel(&#39;Centuries&#39;) plt.ylabel(&#39;Auction Price&#39;) . Text(0,0.5,&#39;Auction Price&#39;) . Yes, it seems it indeed converge to the solution. We can see that, we have taught the machine to learn the value of A, by itself. We can check if the value of A is what we used to generate the value of the dataset . A . Parameter containing: tensor([135078.0156, 53438.6484], requires_grad=True) . It is indeed close. We can find that the value of $a$ is almost the same value as what we used. $b$ being a little different though. But this could have been caused due to the additional random value that we introduced while generating the dataset. . So, now if any new input or sets of inputs, on number of centuries (X) is given, it can find the auction price (Y) for it. . Understanding using Graphs . Now, lets look at the values step by step to understand what was happening . Impact of Iterations in prediction accuracy . from matplotlib import animation, rc rc(&#39;animation&#39;,html=&#39;jshtml&#39;) . A =nn.Parameter(tensor(-1.,1)) fig = plt.figure() plt.scatter(X[:,0],Y,c=&#39;orange&#39;) plt.xlabel(&#39;Centuries&#39;) plt.ylabel(&#39;Auction Price&#39;) line, = plt.plot(X[:,0],X@A) plt.close() def animate(i): update() line.set_ydata(X@A) return line, animation.FuncAnimation(fig,animate,np.arange(0,100),interval=20) . &lt;/input&gt; Once &lt;/input&gt; Loop &lt;/input&gt; Reflect &lt;/input&gt; Now we can clearly see that the arbitrary line that we just drew converged to the exact line that intended line. So, On each iteration of update() we see that the line turns one step closer to where we would intend the line to go towards. All we did was try to minimize the loss function and redraw the line by updating the $A$ value. This moves the line towards where we would like it to move towards. After n iterations (100 here), we see that it the loss becomes minimal and couldn&#39;t find a line that is can have a lower loss than the line above. . Loss Function Vs Value . The Loss Function shows how wrong the prediction is versus actual. Lets see the error rate along different values of $a$, considering $b$ as constant. . b=22000. . mse_list=np.empty(shape=(2,0)) for a in range(124112,154112): A=tensor([a,b]) Y_test=X@A mse_val=mse(Y,Y_test) mse_np=np.array([a,mse_val]).reshape([2,1]) mse_list=np.append(mse_list,mse_np,axis=1) . fig = plt.figure() plt.plot(mse_list[0,:],mse_list[1,:]) plt.xlabel(&#39;a&#39;) plt.ylabel(&#39;Error&#39;) . Text(0,0.5,&#39;Error&#39;) . Now, we can see the value of Error. We can also relate that the value gets towards 0 as we reach an optimum value for $a$. . Loss Function Vs Iterations . Now, lets look how the value of loss goes for this iterations of gradient descent. . def update(): Y_hat = X@A loss = mse(Y,Y_hat) if t %10 ==0: print(loss) loss.backward() with torch.no_grad(): A.sub_(lr * A.grad) A.grad.zero_() return loss . A =nn.Parameter(tensor(-1000.,2)) a_values=np.empty(shape=(2,0)) losses=np.empty(shape=(2,0)) for t in range(100): loss=update() loss_np=np.array([t,loss]).reshape([2,1]) losses=np.append(losses,loss_np,axis=1) a_np=np.array([A[0],mse(Y,X@A)]).reshape([2,1]) a_values=np.append(a_values,loss_np,axis=1) . tensor(1.1503e+10, grad_fn=&lt;MeanBackward1&gt;) tensor(2.2840e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(1.1763e+09, grad_fn=&lt;MeanBackward1&gt;) tensor(9.3902e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.8605e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7420e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7155e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7095e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7082e+08, grad_fn=&lt;MeanBackward1&gt;) tensor(8.7079e+08, grad_fn=&lt;MeanBackward1&gt;) . fig = plt.figure() plt.plot(losses[0,:99],losses[1,:99]) plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Error&#39;) . Text(0,0.5,&#39;Error&#39;) . Now, We could see that the Cost or Error reduces as the number of iterations increase. After a point, we no longer see a decrease in error rate. This error is due to the anonymity. . Note The post is mainly inspired from the Fast AI course. .",
            "url": "https://nareshr8.github.io/ml_posts/pytorch/fast-ai/2019/01/16/linear-regression-coding.html",
            "relUrl": "/pytorch/fast-ai/2019/01/16/linear-regression-coding.html",
            "date": " • Jan 16, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Classify the Classical Dances of India - II",
            "content": "Now that we have downloaded the images here. Our next step is to train the model to learn identifying classical dance images. . Look into the images . First we will import the necessary package . import numpy as np from fastai.vision import * from fastai import * from pathlib import Path . Now that we imported all the libraries we want, we can take the next step. Load the images into various bunches such as training and testing sets. . We usually seed the random so that we can monitor the performance. If we see a improvement in accuracy, it must mean that our tuning worked and not that we were lucky to get the best data in the random shuffle. . np.random.seed(8) PATH= Path(&#39;./data/dances&#39;) data = ImageDataBunch.from_folder(PATH, valid_pct=0.2, train=&quot;.&quot;, size=224,ds_tfms=get_transforms(),num_workers=4).normalize(imagenet_stats) . In the above code . We have made sure the same set of images are used evertime we randomly divide the dataset into training and test set. | We provide the path in which all our images are available | We create a Data Bunch which takes the parameter like PATH path of the images | valid_pct percentage of data that is available in validation set | size size of the images (generally 224*224 is prefered) | ds_tfms the required transformations that are needed. Like trimming, zooming and so on | num_workers the total number of worker threads that are needed to do the computations like transformation | . | We normalise the data so that the images are normalised the same way the pre trained model like resnet34 are normalised | . Now we can do some basic analysis to see if we got the data right . data.classes . [&#39;Bharathanatyam&#39;, &#39;Kathak&#39;, &#39;Kathakali&#39;, &#39;jagoi dance&#39;] . data.show_batch(rows=3, figsize=(7,8)) . . data.classes, data.c, len(data.train_ds), len(data.valid_ds) . ([&#39;Bharathanatyam&#39;, &#39;Kathak&#39;, &#39;Kathakali&#39;, &#39;jagoi dance&#39;], 4, 1957, 496) . Now after some analysis we find that these images like ok. So, leets start by looking into train the machine . Training the model . As we are all set for training the machine, we can continue to train the machine learn the type of dance from image . learn = create_cnn(data=data,arch=models.resnet34, metrics=error_rate) . Note that the method error_rate is available in the package fastai. So, getting the import is essential apart from fastai.vision . We’ll now train the model . learn.fit_one_cycle(4) . Total time: 02:33 epoch train_loss valid_loss error_rate 1 1.074674 0.658487 0.231855 (00:41) 2 0.814936 0.592159 0.211694 (00:39) 3 0.668678 0.592504 0.199597 (00:36) 4 0.570397 0.591368 0.189516 (00:36) . On training the last layer, we get an error of 18% which is good start. We save the model as a checkpoint now. . learn.save(&#39;stage-1&#39;) . Now lets unfreeze all layers and try to make the entire neural network learn . learn.unfreeze() . Now, lets try to identify the learning rate . learn.lr_find() . LR Finder complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot(2) . . Now, we see that somewhere around 3e−63e^-63e−6 to be a good spot . learn.fit_one_cycle(4, max_lr=slice(3e-7,1e-6)) . Total time: 02:29 epoch train_loss valid_loss error_rate 1 0.453784 0.588463 0.191532 (00:37) 2 0.463598 0.588832 0.189516 (00:37) 3 0.472005 0.588780 0.191532 (00:38) 4 0.458501 0.586727 0.189516 (00:36) . learn.save(&#39;stage-2&#39;) . Understanding the model . After we save the model, we will analyse the model so that we can get enough information out of it . learn.load(&#39;stage-2&#39;) . interp = ClassificationInterpretation.from_learner(learn) . Confusion Matrix is a matrix which tabulates the Actual Vs Predicted output. If both are same, then out machine did great job in identifying the image. Lets see how our model preformed . interp.plot_confusion_matrix() . . As the picture says, the machine confuses a lot regarding Bharathanatyam. Well being from the region where it is famus, I might have some knowledge, lets see if thats helpful. . Cleanup . There can be cases where some images are messy. Like, Google can give a search result which has a dancer who is famous in, say Bharathanatyam performing Kathak. These images can confuse the model in learning. Lets look if thats the case . FastAI comes with this very cool widget which is very useful in these cases . from fastai.widgets import * losses,idxs = interp.top_losses() top_loss_paths = data.valid_ds.x[idxs] . We got the list of losses and the top losses that are available. We canuse the widget to look for messy images . fd = FileDeleter(file_paths=top_loss_paths) . HBox(children=(VBox(children=(Image(value=b&#39; xff xd8 xff xe0 x00 x10JFIF x00 x01 x01 x01 x00H x00H x00 x00 xff… Button(button_style=&#39;primary&#39;, description=&#39;Confirm&#39;, style=ButtonStyle()) . Some common pitfalls that we can see is: . Many examples had images of probably famous dances that were taken during some interview or award ceremony | Agenda of the dance festival | The ornaments and dresses thats associated with that dance | The image of artist while they do the make over or dressing while preparing for the dance. | . Now, lets rerun the test. Find the learning rate, train the model… . One thing to note. Since we deleted the images which had these pitfalls, we need to recreate the model with the available data, load the last best model and run these tests. . learn.lr_find() . LR Finder complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot() . . learn.fit_one_cycle(4, max_lr=slice(8e-8,1e-6)) . Total time: 02:31 epoch train_loss valid_loss error_rate 1 0.508052 0.322376 0.106029 (00:38) 2 0.501633 0.319395 0.106029 (00:36) 3 0.498382 0.320848 0.106029 (00:36) 4 0.497239 0.319271 0.112266 (00:38) . Now, our new model looks better. Has around 11.2% errors, which is like 88.8% accurate. That looks pleasing. Can we do better? Will update the blog if I do so. .",
            "url": "https://nareshr8.github.io/ml_posts/fast-ai/icrawler/2018/11/08/learn-identifying-classical-dances.html",
            "relUrl": "/fast-ai/icrawler/2018/11/08/learn-identifying-classical-dances.html",
            "date": " • Nov 8, 2018"
        }
        
    
  
    
        ,"post4": {
            "title": "Classify the Classical Dances of India - I",
            "content": "There are various classical dances that are available all throughout in India. These classical dances are culture specific. For example, Tamil Nadu follows a classical dance called Bharathanaatiyam where as its nearby state, Kerala has a classical dance of Kathakali. . Being not much knowledgable in any of the classical dances, I was triggered by the idea of Jeremy Howard who was suggesting that we might be able to solve deep learning problems even if we ourselves arent the domain experts. So, I decided to try myself out if I could build a model that can perform well in classifying the classical dance names. . Steps . To build a classifier, we have to build a classifier on our own that can be run end to end, we need to follow the below steps. . Download the dataset | Do preprocessing of data, if any | Create a model to detect the classical dance | . I prefer installing the libraries usiing the notebook itself so that we can reuse it anytime as a package. . I use icrawler library to download data from the google and bing. Also FastAI library has this feature to check and delete the files which have the corrupted images. . # !pip install icrawler # !pip install fastai . Now that we have installed all the required libraries, we could start downloading the dataset. . from icrawler.builtin import GoogleImageCrawler from fastai.vision import * from pathlib import Path from random import randint import logging . We need to download images of few types of classical dance images for training the model. . classes = [&#39;Bharathanatyam&#39;,&#39;Kathakali&#39;,&#39;Kathak&#39;,&#39;jagoi dance&#39;] . After we downloaded the images, we need to check if the downloaded images have some corrupt images as well. These images will not open for some reason and we dont want to train our network with these images. . Fast AI comes with a method to verify these images using verify_images method . for c in classes: print(c) download_directory = &#39;./data/dances/&#39;+c search_filters_google = dict() google_crawler = GoogleImageCrawler(downloader_threads=4,storage={&#39;root_dir&#39;: download_directory}) google_crawler.crawl(keyword=c, filters=search_filters_google, max_num=1000) verify_images(download_directory, delete=True, max_workers=8) . Now that we have downloaded the images we first check if the folder is created correctly . !ls data/dances . Bharathanatyam jagoi dance Kathak Kathakali . Now we can randomly check if the files are created correctly . !ls data/dances/&#39;{classes[3]}&#39; . # !rm -r data/dances/* . path = Path(&#39;data/dances/&#39;+classes[randint(0,3)]) print(path.parts[-1]) pathiter = list(path.iterdir()) open_image(pathiter[randint(0,len(pathiter))]) . Kathak . . Note: This blog post is completely written in Jupyter notebook. Though, Some outputs that might be not suitable for blog reading are removed. . To create a model that learns from these images, look into the next part here .",
            "url": "https://nareshr8.github.io/ml_posts/fast-ai/icrawler/2018/11/08/download-classical-dance.html",
            "relUrl": "/fast-ai/icrawler/2018/11/08/download-classical-dance.html",
            "date": " • Nov 8, 2018"
        }
        
    
  
    
        ,"post5": {
            "title": "Understanding of Fast AI - Course V3 (Part 1 - Lesson 1)",
            "content": "This is the notes of first lesson of the list of lessons in the Part 1 of Fast AI V3 course. Though I went though few lessons on Fast AI course of last year, I was sure to do this course. This is the first course that comes after the first version release of Fast AI library itself. Last year it was in Beta Version (think 0.7 when I took it online) and it is now into version 1.0.14. . What’s your pet . In this lesson we will build our first image classifier from scratch, and see if we can achieve world-class results. Let’s dive in! . The lines in jupyter notebook that starts with ‘%’ are called Line Magics. These are not instructions for Python to execute, but to Jupyter notebook. . They ensure that any edits to libraries you make are reloaded here automatically, and also that any charts or images displayed are shown in this notebook. . The reload_ext autoreload reloads modules automatically before entering the execution of code typed at the IPython prompt. . The next line autoreload 2 imports all modules before executing the typed code. . The documentation on autoreload is available here . The next line is to plot the graphs inside the jupyter notebook. We use matplotlib inline. . %reload_ext autoreload %autoreload 2 %matplotlib inline . We import all the necessary packages. We are going to work with the fastai V1 library which sits on top of Pytorch 1.0. The fastai library provides many useful functions that enable us to quickly and easily build neural networks and train our models. . from fastai import * from fastai.vision import * . Looking at the data . We are going to use the Oxford-IIIT Pet Dataset by O. M. Parkhi et al., 2012 which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate “Image”, “Head”, and “Body” models for the pet photos. Let’s see how accurate we can be using deep learning! . We are going to use the untar_data function to which we must pass a URL as an argument and which will download and extract the data. . help(untar_data) . Help on function untar_data in module fastai.datasets: untar_data(url: str, fname: Union[pathlib.Path, str] = None, dest: Union[pathlib.Path, str] = None, data=True) Download `url` if doesn&#39;t exist to `fname` and un-tgz to folder `dest` . The untar_data is great idea for downloading the URL provided and download and untar. . path = untar_data(URLs.PETS); path . PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet&#39;) . In Python, we can extend and add new functionalities to the existing python modules. I found this link useful for creating one. . path.ls() . [PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/annotations&#39;)] . The pathlib thats part of Python 3 has the notation / which is useful to navigate into the directory as in the actual directory. We use to create Path variables with the new location. . path_anno = path/&#39;annotations&#39; path_img = path/&#39;images&#39; . Lets check all the images in the image directory . fnames = get_image_files(path_img) fnames[:5] . [PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/leonberger_34.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/pug_203.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/Siamese_203.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/scottish_terrier_98.jpg&#39;), PosixPath(&#39;/home/nbuser/courses/fast-ai/course-v3/nbs/data/oxford-iiit-pet/images/beagle_76.jpg&#39;)] . np.random.seed(2) pat = r&#39;/([^/]+)_ d+.jpg$&#39; . The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like. . The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, ImageDataBunch.from_name_re gets the labels from the filenames using a regular expression. A detailed explaination found on an interesting read about the same lines of code in this tutorial is here. . Loading Images: . ImageDataBunch is used to do classification based on images. We use the method from_name_re to represent that the name of the classification is to be got from the name of the file using a regular expression. It takes the following parameters: . path_img the path of the images directory | fnames the list of files in that directory | pat the regex pattern that is used to extract label from the file name | ds_tfms the transformations that are needed for the image. This includes centering, cropping and zooming of the images. | size the size to which the image is to be resized. This is usually a square image. This is done because of the limitation in the GPU that the GPU performs faster only when it has to do similar computations (such as matrix multiplication, addition and so on) on all the images. | . Data Normalisation: . This is done to ensure that the images are easy to do mathematical calculations that we are looking for after that. This includes changing the range of values of RGB from 0-255 to -1 to 1.This is because we have 3 color channels namely Red, Green and Blue. For the pixel values we might have some color channels that varies slightly and some that doesn’t. So, we need to normalise the images with mean as 0 and standard deviation as 1. . One another thing to note is that we are using the residual network, which is pretrained. So, we must use the same normalisation that the residual network is using in order to use the best of the pretrained model. . data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224) data.normalize(imagenet_stats) . &lt;fastai.vision.data.ImageDataBunch at 0x7f21d8961eb8&gt; . Now, we can look into the image samples along with the classification name to check if everything that we have done thus far is doing great. . Its important to check this as we may understand some images might have some kind of issue over the other, like rotated in odd ways, just text on it, 2 different categories of classificatiers on it and so on. . doc(ImageDataBunch) . data.show_batch(rows=3, figsize=(7,6)) . . We use the data.classes to indicate the total number of distinct labels that were found. In our case, since wwe have extracted the labels from the regular expression, it indicates the number of distinct labels that were extracted from the regular expression. . data.c gives the total number of classifications that were found in the dataset . print(data.classes) len(data.classes),data.c . [&#39;leonberger&#39;, &#39;pug&#39;, &#39;Siamese&#39;, &#39;scottish_terrier&#39;, &#39;beagle&#39;, &#39;Birman&#39;, &#39;Abyssinian&#39;, &#39;great_pyrenees&#39;, &#39;chihuahua&#39;, &#39;havanese&#39;, &#39;japanese_chin&#39;, &#39;yorkshire_terrier&#39;, &#39;Persian&#39;, &#39;Ragdoll&#39;, &#39;pomeranian&#39;, &#39;newfoundland&#39;, &#39;Bombay&#39;, &#39;shiba_inu&#39;, &#39;german_shorthaired&#39;, &#39;Bengal&#39;, &#39;samoyed&#39;, &#39;boxer&#39;, &#39;wheaten_terrier&#39;, &#39;miniature_pinscher&#39;, &#39;english_cocker_spaniel&#39;, &#39;Maine_Coon&#39;, &#39;Sphynx&#39;, &#39;British_Shorthair&#39;, &#39;staffordshire_bull_terrier&#39;, &#39;keeshond&#39;, &#39;saint_bernard&#39;, &#39;american_pit_bull_terrier&#39;, &#39;Russian_Blue&#39;, &#39;american_bulldog&#39;, &#39;english_setter&#39;, &#39;Egyptian_Mau&#39;, &#39;basset_hound&#39;] (37, 37) . Training: resnet34 . Now we will start training our model. We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. Since we are using Residual Network with 34 hidden units, all we have to do is to add a layer at the end on the residual network to transform the dimension of the residual network to the required output. In our case, it is to the 37 possible outputs. . We will train for 4 epochs (4 cycles through all our data). . We create a learner object that takes the data, network and the metrics . The metrics is just used to print out how the training is performing. We choose to print out the error_rate. . learn = create_cnn(data, models.resnet34, metrics=error_rate) . Downloading: &quot;https://download.pytorch.org/models/resnet34-333f7ec4.pth&quot; to /home/nbuser/.torch/models/resnet34-333f7ec4.pth 100%|██████████| 87306240/87306240 [00:02&lt;00:00, 29535503.58it/s] . learn.fit_one_cycle(4) . Total time: 03:19 epoch train_loss valid_loss error_rate 1 1.156555 0.291909 0.091151 (00:53) 2 0.505356 0.249506 0.077179 (00:48) 3 0.312138 0.212065 0.074518 (00:49) 4 0.240234 0.198288 0.069195 (00:48) . learn.save(&#39;stage-1&#39;) . Results . Let’s see what results we have got. . We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. . Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour. . interp = ClassificationInterpretation.from_learner(learn) . interp.plot_top_losses(9, figsize=(15,11)) . . doc(interp.plot_top_losses) . interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . . interp.most_confused(min_val=2) . [(&#39;Ragdoll&#39;, &#39;Birman&#39;, 6), (&#39;staffordshire_bull_terrier&#39;, &#39;american_pit_bull_terrier&#39;, 5), (&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 5), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 5), (&#39;Birman&#39;, &#39;Ragdoll&#39;, 4), (&#39;British_Shorthair&#39;, &#39;Russian_Blue&#39;, 4), (&#39;Bengal&#39;, &#39;Egyptian_Mau&#39;, 3), (&#39;english_cocker_spaniel&#39;, &#39;english_setter&#39;, 3), (&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 3), (&#39;american_pit_bull_terrier&#39;, &#39;american_bulldog&#39;, 3), (&#39;american_bulldog&#39;, &#39;staffordshire_bull_terrier&#39;, 3)] . Unfreezing, fine-tuning, and learning rates . Since our model is working as we expect it to, we will unfreeze our model and train some more. . learn.unfreeze() . learn.fit_one_cycle(1) . Total time: 01:07 epoch train_loss valid_loss error_rate 1 1.070711 0.524961 0.168995 (01:07) . Since the Model underperformed while training after unfreeze, we would like to move to our previous best model that we have saved stage-1. We will finetune to improve from here. . learn.load(&#39;stage-1&#39;) . We use the lr_find method to find the optimum learning rate. Learning Rate is an important hyper-parameter to look for. We traditionally use $ alpha$ to denote this parameter. If the Learning rate is too slow, we take more time to reach the most accurate result. If it is too high, we might not even end up reaching the accurate result. Learning Rate Finder was idea of automatically getting the magic number (which is near perfect), to get this optimum learning rate. This was introducted in last year’s Fast AI course and continues to be useful. . learn.lr_find() . LR Finder complete, type {learner_name}.recorder.plot() to see the graph. . After we run the finder, we plot the graph between loss and learning rate. We see a graph and typically choose a higher learning rate for which the loss is minimal. The higher learning rate makes sure that the machine ends up learning faster. . learn.recorder.plot() . . We see around $1e^{-4}$ mark, we have a optimum learning rate. Now that we know the optimum learning rate. . Considering the fact that we are using a pretrained model of resnet-34, we know for sure that our previous layers of this neural network would learn to detect the edges and the later layers would learn complicated shapes such as the dogs and cats itself. We don’t want to ruin out the earlier layers which presumably does a good job of detecting the edges. But would like to improve the model in narrowing down classifying the image of dogs and cats to our needs, which is done in the later layers. . So, we will set a lower learning rate for earlier layers and higher one for the last layers. . The slice is used to provide the learning rate wherein, we just provide the range of learning rates (its min and max). The learning rate is set gradually higher as we move from the earlier layer to the latest layers. . learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4)) . Total time: 02:14 epoch train_loss valid_loss error_rate 1 0.199850 0.185868 0.064538 (01:07) 2 0.194062 0.182656 0.065203 (01:07) . That’s a pretty accurate model! . Training: resnet50 . Now we will train in the same way as before but with one caveat: instead of using resnet34 as our backbone we will use resnet50 (resnet34 is a 34 layer residual network while resnet50 has 50 layers. Later in the course you can learn the details in the resnet paper). . Basically, resnet50 usually performs better because it is a deeper network with more parameters. Let’s see if we can achieve a higher performance here. . data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=299, bs=48) data.normalize(imagenet_stats) . &lt;fastai.vision.data.ImageDataBunch at 0x7f21d89b3860&gt; . learn = create_cnn(data, models.resnet50, metrics=error_rate) . learn.fit_one_cycle(5) . Total time: 16:40 epoch train_loss valid_loss error_rate 1 0.646226 0.257891 0.085036 (03:53) 2 0.348598 0.244830 0.082399 (03:11) 3 0.236005 0.192446 0.061964 (03:11) 4 0.149788 0.147233 0.044825 (03:11) 5 0.100550 0.138161 0.048121 (03:11) . learn.save(&#39;stage-1-50&#39;) . It’s astonishing that it’s possible to recognize pet breeds so accurately! Let’s see if full fine-tuning helps: . learn.unfreeze() learn.fit_one_cycle(1, max_lr=slice(1e-6,1e-4)) . Total time: 04:27 epoch train_loss valid_loss error_rate 1 0.088951 0.151667 0.050758 (04:27) . In this case it doesn’t, so let’s go back to our previous model. . learn.load(&#39;stage-1-50&#39;) . We now load the previous best model and would like to improve upon that model. . interp = ClassificationInterpretation.from_learner(learn) . interp.most_confused(min_val=2) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 6), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 5), (&#39;Egyptian_Mau&#39;, &#39;Bengal&#39;, 5), (&#39;Ragdoll&#39;, &#39;Persian&#39;, 4), (&#39;staffordshire_bull_terrier&#39;, &#39;american_bulldog&#39;, 4), (&#39;Maine_Coon&#39;, &#39;Ragdoll&#39;, 3)] . learn.lr_find() . LR Finder complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot() . . learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4)) . Total time: 08:26 epoch train_loss valid_loss error_rate 1 0.116429 0.138112 0.049440 (04:17) 2 0.084291 0.129927 0.044166 (04:08) . learn.save(&#39;stage-2-50&#39;) . Save the model as it seems a little more accurate . Other data formats . path = untar_data(URLs.MNIST_SAMPLE); path . PosixPath(&#39;/home/jhoward/.fastai/data/mnist_sample&#39;) . tfms = get_transforms(do_flip=False) data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26) . data.show_batch(rows=3, figsize=(5,5)) . . learn = ConvLearner(data, models.resnet18, metrics=accuracy) learn.fit(2) . VBox(children=(HBox(children=(IntProgress(value=0, max=2), HTML(value=&#39;0.00% [0/2 00:00&lt;00:00]&#39;))), HTML(value… Total time: 00:11 epoch train loss valid loss accuracy 1 0.108823 0.025363 0.991168 (00:05) 2 0.061547 0.020443 0.994112 (00:05) . df = pd.read_csv(path/&#39;labels.csv&#39;) df.head() . name label . 0 train/3/7463.png | 0 | . 1 train/3/21102.png | 0 | . 2 train/3/31559.png | 0 | . 3 train/3/46882.png | 0 | . 4 train/3/26209.png | 0 | . data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28) . data.show_batch(rows=3, figsize=(5,5)) data.classes . [0, 1] . . data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24) data.classes . [0, 1] . fn_paths = [path/name for name in df[&#39;name&#39;]]; fn_paths[:2] . [PosixPath(&#39;/home/jhoward/.fastai/data/mnist_sample/train/3/7463.png&#39;), PosixPath(&#39;/home/jhoward/.fastai/data/mnist_sample/train/3/21102.png&#39;)] . pat = r&quot;/( d)/ d+ .png$&quot; data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24) data.classes . [&#39;3&#39;, &#39;7&#39;] . data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24, label_func = lambda x: &#39;3&#39; if &#39;/3/&#39; in str(x) else &#39;7&#39;) data.classes . [&#39;3&#39;, &#39;7&#39;] . labels = [(&#39;3&#39; if &#39;/3/&#39; in str(x) else &#39;7&#39;) for x in fn_paths] labels[:5] . [&#39;3&#39;, &#39;3&#39;, &#39;3&#39;, &#39;3&#39;, &#39;3&#39;] . data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24) data.classes . [&#39;3&#39;, &#39;7&#39;] . .",
            "url": "https://nareshr8.github.io/ml_posts/fast-ai/2018/10/31/fastai-p1-l1.html",
            "relUrl": "/fast-ai/2018/10/31/fastai-p1-l1.html",
            "date": " • Oct 31, 2018"
        }
        
    
  
    
        ,"post6": {
            "title": "Getting Started in Machine Learning - Basics",
            "content": "Machine Learning - Basics . What is Machine Learning? . There are many perspectives on what machine learning is. But a well accepted definition is : . A Field of study that gives computers the ability to learn without being explicitly programmed - Arthur Samuel (1959) . A slightly complicated to understand but accurate one would be . A computer program is said to learn from experience E with respect to task T and some performance measure P, if its performance on T, as measured by P, improves with experience E - Tom Mitchell (1998) . Types of Machine Learning . There are most of the machine learning concepts some in one among any of the carders: . Supervised Learning - | Unsupervised Learning | Reinforcement Learning | Recommender Systems | . Supervised Learning . Supervised learning is a learning technique used when we have a sample data with a given set of inputs and its corresponding ‘expected value’, we expect the computer to predict the ‘expected value’ when a new set of inputs is given to it. . Example: . We would take the most common example of Housing Price prediction. We would be given a list of ‘features’ of the house, area of the house and the cost of the house in that area, Our goal would be to predict the price of the house for an unseen house for which we know the height and width. . . Now, we mark the area of the house (X-axis) along with its price (Y-axis) plotted in a graph as a (X) mark. . Now, what we do by the process called training, is to draw a function f(x) which maps x to y. In essence, y=f(x) is the function we will try to come up with so that for any new value of x, we can find the value of y by a process called predict. Here the list of features ‘x’ is called independent variance and the expected output ‘y’ is the dependent variable. Here the expected value ‘y’ is called independent variable. . If the independent variable is continuous valued, the type of supervised machine learning is called regression. The above one is a good example of regression based supervised machine learning. . The contradictory example is to predict the type of cancer, Malignant or Benign based on the size of the tumor. Here, the size of the tumor is the independent variable whereas, Cancer type is the dependent variable and can take one of the two values (Malignant or Benign) only. Hence this type of machine learning is called classification. . . Unsupervised Learning . Unsupervised Learning is another category of Machine Learning where we would not be given the right set of answers. Instead, we give a set of data and ask the system, can you find a pattern in this data. . Example . One example of the unsupervised learning might be customer segmentation. We give give the system the customer segments and look for the system to segment out the customers without explicitly saying which segment any customer belongs to. . . Reinforcement Learning . Reinforcement Learning is a category of machine learning where the machine learns to perform a Task in an environment so as to maximize the reward over the long term. . A famous example might be the Alpha Go, developed by Google which bet the professional Go players. . Recommendation Systems . Recommendation systems are a class of machine learning which would predict the rating or preference of the user to a given item. . Example . Common examples are recommended products that are available in almost all online shopping sites such as Amazon, Flipkart, Ebay. . . Disclaimer: Most part of the contents in this blog are from the Machine Learning course by Andrew Ng. .",
            "url": "https://nareshr8.github.io/ml_posts/ml/2018/10/02/ml-basics.html",
            "relUrl": "/ml/2018/10/02/ml-basics.html",
            "date": " • Oct 2, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Object Localization",
            "content": "Today, we work for a particular problem statement, image localization. Image Localization basically means that we will look into a given image and not just say whether a stamp is available in the picture or not but also where it is, if available. . The problem statement is easy for a kid to perform, but was impossible few years back for even super computers to accurately say like a kid. Thanks to the advancement in computational power and deep learning literature, we are now able to make computers do this task. . We are using a python library “Fast AI” to perform this task. If you are little unfamiliar with Fast AI, checkout fast.ai. It’s the easiest way to get your hands on machine learning and do some exciting tasks straight up. . Problem Statement . Your company sends the manufactured products out of the garage only when they have been tested. The tested products have a seal like this: . You are a data scientist or a machine learning engineer who is trying to use computers to check if the product is tested or not instead of humans, as it speeds up the disposal of the products. . Why Data Augmentation . Most of the time the company doesnt have enough images to perform machine learning tasks on. This is also a typical case in your company. Your company can only give you the image of the stamp. You try to collect the data yourself. However, You need a camera to take pictures on the product’s cover art so that you can collect thousands of images. Your company asks for a proof that your product will work before it can install the camera on the product disposal area. This lands you in a place where you have to get the product working before the main component of the product (data) is available. So, we have to augment the data. . Data Augmentation . We plan to augment whatever data is needed. We crawl through the web and pull images of various products and superimpose this stamp over them to create data that we are looking for. We thereby have a set of images that is available with/without stamp and the location if its available. . Image Crawling . For image crawling in python we have a library ‘icrawler’, which I have used and seem to pull images from different data sources like Google, Bing, Baidu. We can use the crawler to pull the images from internet. The working crawler notebook is available here. . Why crawl thousands of images? . If we have only 10 images in training set as background and superimpose the stamp in various locations, there is a possibility of overfitting, meaning that the machine will try to remove the 10 possible backgrounds and check if any of the remaining pixel is having the value, instead of looking at the stamp. Which will not be the ideal scenario for the actual images. So, even if its augmentation, the more data the better. . Image Super-Imposition . I have used PIL library to super impose the images. The image preprocessing is done in this notebook. . Training the Model . For training the model, we train the model using the notebook that is available here. . Other variants . Here we used only one stamp and we are checking if that particular stamp is available. Instead, we can generate data with all the variety of labels with all variants of stamps as well and pretty much follow the same procedure. We would be able to train the model to check for any of the stamps is available and the location. The other alternative is Person Tagging. Similar to stamps, we can give a set of images with people faces and ask the machine to tag the person’s face on the image. . Improvisation . As part of improvising, we can . tag multiple items on the same image. | identify each class of the tagged image. | Use this process as part of a end to end solution. For example, if we want to know the price of a product, we naturally find the location where the price is listed. After localizing the place where the price is listed, we try to read the price. We can do the same with machines to understand the price of the product. | Use to develop Optical Character Recognition. We tag each character and a classifier that classifies between A-Z and Numbers. The characters with lesser space between them forms a word. | . I may try some of these myself and post if something really cool works out. . Post your comments and let know your views on this. .",
            "url": "https://nareshr8.github.io/ml_posts/object-localisation/cnn/resnet/fast-ai/fastai/2018/08/15/object-localisation.html",
            "relUrl": "/object-localisation/cnn/resnet/fast-ai/fastai/2018/08/15/object-localisation.html",
            "date": " • Aug 15, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Preprocessing Structured Data For Machine Learning - I",
            "content": "Preprocessing the data is the first part of any machine learning project that we take up. This blog post, being my first ever, will start to discuss about preprocessing of data in python. I used Jupyter Notebook/lab as the IDE of preference along side Python 3. . Why Data Preprocessing? . Data Preprocessing ensures that the data is available in the right format for machine learning to be performed. . The golden rule for any machine learning project is more the data, the better. So, We might collect data from various sources. All data may not be good straight away for starting machine learning. We may run into one or many of these problems, most of the time: . Missing Data - Some columns might be missing in some rows. | Incorrect Data - Some data might have been wrong due to manual entry or inconsistent data source | Feature Scaling - Algorithms such as KNN, SVM prefer uniform distribution among the dataset as it uses distance or similarities between datasets. | . Now that the need of preprocessing is felt, we can start to preprocess the data. Here, the theme is to get started building a notebook with functions that are commonly used in preprocessing structured data that can be used for any structured data. . Prerequisites . We are using the Following libraries for making our process easier. . Pandas . To install run this on your python console : . !pip install pandas . ​ . | Numpy . To install run this on your python console : . !pip install numpy . | . A simple google might solve the problem if you have any trouble installing these packages. . Preprocessing Steps . First we have to import our necessary packages . import numpy as np import pandas as pd . Now collect the data into Data Frame. We can collect data from different sources into a Data Frame. Since we are creating the just utilities that take in data frame as input, we can skip this step. . Getting list of Missing Values . We may have to deal with the fact that there will be missing values in one or more columns in our dataset. First we may have to take a look on how many columns have missing values. So we can have a helper method do that for us. . def get_missing_valued_columns_list(dataset): return dataset.columns[dataset,isnull().any()] . This gives the list of columns which has missing values. . Now that we see the list of columns which has missing values, we might be interested in knowing how many values among those are missing. If there are columns that misses more than, say 80% of data, we might chose to ignore that column. . Getting List of Columns with missing count . Now we look to get the data that shows the list of columns with the count of data that is missing in those column . def get_missing_valued_column_details(dataset): sum_of_missing_values = dataset.isnull().sum(axis=0) return sum_of_missing_values[sum_of_missing_values &gt; 0] . Here, we first summed up the count of data that has values as null (along the vertical axis), then we filtered out non empty columns. . Get labels which doesn’t have enough data . If our label doesn’t have enough data, we cannot make useful predictions out of the data. Thus, can remove columns that doesn’t have enough data in them. . def get_low_variance_columns(dataset): from sklearn.feature_selection import VarianceThreshold columns = dataset.iloc[:,:-1].columns selector = VarianceThreshold(.8 * (1-.8)) selector.fit(dataset.iloc[:,:-1]) labels = [columns[x] for x in selector.get_support(indices=True) if x] return labels . Here we are selecting all columns except the last one, as last column is usually the output or prediction column. Then we use VarianceThreshold function thats part of scikit learn library to specify the threshold (.8) to retrieve the list of columns. . I am planning to add other commonly used functions in my next blog post. Comment on your views. .",
            "url": "https://nareshr8.github.io/ml_posts/preprocessing/machine-learning/2018/05/21/data-pre-processing-i.html",
            "relUrl": "/preprocessing/machine-learning/2018/05/21/data-pre-processing-i.html",
            "date": " • May 21, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Application Developer at Tata Consultancy Services . Developing Machine Learning, Java and Native Android Applications. . Have worked under various clients such as Walmart, Home Depot, Century Link, Qantas, Nielsen, Vodafone Hutchison Australia since Nov 2011. . Education . M.Tech in Software Systems - BITS Pilani (2013-2015) . B.Tech in Information Technology - Anna University (2007-2011) . Hobbies . Playing Cricket, Listening Music, watching YouTube and now starting to blog . Programming . I ♥ programming. I love to code and develop interesting stuff myself. . I have developed passion towards machine learning and started working for more than 2 years. Have been certified for Deep Learning Specialisation and Advanced Machine Learning with TensorFlow on Google Cloud Platform in Coursera. . Also, I am also certified in Android Application Development .I have developed few android apps myself and put on Play Store .",
          "url": "https://nareshr8.github.io/ml_posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}