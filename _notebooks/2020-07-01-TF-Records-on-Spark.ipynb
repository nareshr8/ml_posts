{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-Records on Spark Cluster\n",
    "> Creating and Reading TF Records on Spark Cluster\n",
    "\n",
    "- comments: true\n",
    "- toc: true\n",
    "- badges: true\n",
    "- categories: [TF-Records, Spark, Py-Spark, Performance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxZuGi7zXtG3"
   },
   "source": [
    "TF-Records abbreviated as TensorFlow Records is one the data formats that has serveral benefits such as performance (time) of the Tensorflow training. Using just TF-Records, I was able to get a direct decrease in the training time 3x times.\n",
    "\n",
    "Most of the example blogs on TF-Records however, had images as example. I was working on a usecase of training a tabular data on spark cluster. I am posting this blog for easing out TF-Records adoption for scenarios such as mine. Hope this is useful.\n",
    "\n",
    "\n",
    "> Note: The intention of this post is about how to use TF-Records for better performances. We could run the training distributed across the cluster. Tensorflow API supports distributed training. We could also try `Horovod` which uses `Petastorm` file format to train the data in the spark cluster. But those topics are beyond the scope of this post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJ3wYJuY1BkX"
   },
   "source": [
    "Some Advantage of TF-Records are that, it :\n",
    "- serialises data and stores. This means reduced space requirement for storage, faster data read and copy\n",
    "- uses [Protocol Buffer](https://developers.google.com/protocol-buffers) format to store data which makes reading of data faster. \n",
    "- loads only required data into the memory. This is useful especially for large datasets\n",
    "- The data is never brought to Python level and is always dealt with C++ level which makes training faster\n",
    "- Tensorflow moves the data to GPU while training is performed\n",
    "\n",
    "But to save/retrieve the records in TF-Records format, we need to have schema information to it which makes the process of creating TF-Records different from CSV / Excel / SQL. \n",
    "\n",
    "> Note: CSV/Excel doesn't need even column names to both read and write. SQL doesn't need datatype for querying however might need column name if we need only a specified column. This is not the case for TF Records. It need both name and its data-type for both write and read operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSl6nUKZZ0as"
   },
   "source": [
    "## Creating TF-Records\n",
    "\n",
    "The first step is to write the data in the desired TF-Records format. The basic way to create TF-Records is to use `tf.python_io.TFRecordWriter` API. But there is a simpler way to create TF-Records in Spark cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YX3mPasDffFS"
   },
   "source": [
    "### Library required\n",
    "Creating the TF-Records in Spark cluster is easy. Thanks to the Spark Tensorflow Connector (`spark-tensorflow-connector_2.11-1.10.0.jar`).\n",
    "> Note: We are skipping the part on how we install this library into the cluster. This is a JAR install on the cluster and is usually generic. Also note that this library is needed only needed to create TF-Records. since we use Tensorflow (`tf.data` API) to read the data, we might not have to bother installing this library if we are using TF-Records created else where."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJCnbY6Y0x6k"
   },
   "source": [
    "### Writing the TF-Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eBBBhLYcYMZ"
   },
   "source": [
    "The API is so neat and simple to create TF-Records. Since the schema can be infered from the dataframe itself, we need not provide the same to write the data. This might not be the case when we use the traditional `TFRecordWriter`. \n",
    "\n",
    "Suppose you have a spark-dataframe `preprocessed_df`. The easiest way to create the TF-Records is :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UN82QMikXqdS"
   },
   "outputs": [],
   "source": [
    "preprocessed_df.write.format('tfrecords').save(path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6ARx01Hu9Xg"
   },
   "source": [
    "We could save the data in an actual spark table as a backup.\n",
    "\n",
    "> Tip: We might have to read the data from TF-Records for non tensorflow purpose as well (like data analysis). But it is typically slow to read the data (even using `spark.read` API)  compared to saving it as spark table (as `parquet` files). So I have a copy of the data in spark table and one in TF-Records format. I am saving to a seperate spark table that I use for analysis of data. I am using the same table to infer schema for retrieving the TF-Records. If you dont like to dump the data, all you need is the list of columns and its type to decode the data in the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzSfF0CpcAA-"
   },
   "outputs": [],
   "source": [
    "preprocessed_df.write.format('parquet').mode('overwrite').saveAsTable(preprocessed_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4O9USZnxgC_6"
   },
   "source": [
    "## Reading TF-Records\n",
    "\n",
    "To read the TF-Records for usage in tensorflow, we can use the `tf.data` API.\n",
    "\n",
    "As discussed, TF-Records have a catch that to read the data from TF-Record files, we need to know the schema of the data to read/decode the files. Since we have spark table stored, we infer its schema from the spark table that we already saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WARWqQY4mNl5"
   },
   "source": [
    "### Infering Schema\n",
    "To read the records we need to have the list of features and their types. We could use the backup spark table's schema to get that information. So we just read the spark table that we already stored to infer its schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCsrIViBhrak"
   },
   "outputs": [],
   "source": [
    "preprocessed_df = spark.read.table(preprocessed_table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKyhPO4awOM_"
   },
   "source": [
    "We collect the schema dtypes as key/value pair into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOiPFnuYv_Vi"
   },
   "outputs": [],
   "source": [
    "column_dtypes = {col:dtype for col,dtype in preprocessed_df.dtypes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8S9cgrGwm8X"
   },
   "source": [
    "### Create features from column dictionary\n",
    "\n",
    "Now that we have the column dictionary, we have to create a features dictionary where we specify if the data is *FixedLenFeature* (typically for mandatory data) or *VarLenFeature* (typically for optional variables) and its data-type. I use `FixedLenFeature` as I dont have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-0DmTzUryuJx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Since we have all fixed length features, we create a lambda helper function to create features \n",
    "_fixed_feature = lambda x: tf.io.FixedLenFeature([],x)\n",
    "\n",
    "def create_features(dtype_dict):\n",
    "  features={}\n",
    "  for dtype_tup in dtype_dict.items():\n",
    "    if dtype_tup[1] in ('int','bigint','integer'):\n",
    "      features[dtype_tuple[0]] = _fixed_feature(tf.int64)\n",
    "    elif dtype_tup[1] in ('double','float','long'):\n",
    "      features[dtype_tuple[0]] = _fixed_feature(tf.float32)\n",
    "    elif dtype_tup[1] in ('string'):\n",
    "      features[dtype_tuple[0]] = _fixed_feature(tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vIx2ifi0DV4"
   },
   "source": [
    "As we can create features using the above convenient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPn7G0CL0Rxv"
   },
   "outputs": [],
   "source": [
    "features = create_features(column_dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPyBJXdV5R7b"
   },
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBVfVsJY05Xt"
   },
   "source": [
    "Having created the features we could decode the TF-Records. To simplify the process we create a decode method.\n",
    "\n",
    "Initially I used `tf.io.Example` API to decode. It down performed the training time as compared to training from CSV file, since it decodes one record at a time.\n",
    "\n",
    "> Important: If you used the common `tf.io.Example` API, you might face performance lag in decoding. This is because the data is decoded one record at a time. use of `tf.io.parse_example` will parse the data of the entire batch (refer the [documentation](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) for more understanding).\n",
    "\n",
    "\n",
    "\n",
    "It is to be noted that while decoding the serial data we must return a tuple of independent and dependent variables. But the data we stored doesnt have the knowledge of which columns constitute to both. Hence we might have to handle that in our decode method  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgE9GnDc1GTJ"
   },
   "outputs": [],
   "source": [
    "output_cols={} # Specify the single or list of columns that are dependent variables\n",
    "def decode(serial_data):\n",
    "  # We use `parse_example` instead of `Example` for decoding in batches\n",
    "  parsed_data = tf.io.parse_example(serialized=serial_data,features=features)\n",
    "  # We segregate the dependent variables seperately for returning the appropriate tuple\n",
    "  y = {col:parsed_data.pop(col) for col in output_cols}\n",
    "  return parsed_data, tf.transpose(a=list(y.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPZn2ff_8KUL"
   },
   "source": [
    "### Read data using `tf.data`\n",
    "If we have created the data using spark, the files are put in the folder which we specify in the `save` method. This creates multiple TF-Record files. But it also contains the `_SUCCESS` file along with it. We might have to ignore that file while providing data to the TF Data API to avoid a potential error. This can be done using a simple method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rzQQzl6fa7l"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def get_tf_records(folder_path):\n",
    "  return [i for i in map(str,Path(folder_path).iterdir()) if i.name != '_SUCCESS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbnqJyJtfCho"
   },
   "source": [
    "Now, we can simply use this method to get all TF-Record files in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NoOwHpyY7YVN"
   },
   "outputs": [],
   "source": [
    "training_records = get_tf_records(path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXSGCalvTwFO"
   },
   "source": [
    "The next part is to feed the data into the `tf.data` API. The API has a built-in method `tf.data.Dataset.from_tensor_slices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AdPfszkNTupV"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(training_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_vLQVBlZRyS"
   },
   "source": [
    "\n",
    "> Note: `tf.data` is an excellent API. It has lots of features that can improve the performance of the training. Some of the parameters like `num_parallel_calls`,`block_length`,`cycle_length` were useful in particular. The `shuffle` API allows us to provide a number of items to be picked up and shuffled to get `batch_size` items. The details on tuning `tf.data` for performance must be a post by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPPD6vcs7Adm"
   },
   "source": [
    "### Training the module\n",
    "\n",
    "To train the module we need to:\n",
    "- Get the dataset\n",
    "- Batch\n",
    "- Decode\n",
    "- Prefetch (not mandatory, just a performance tweak)\n",
    "\n",
    "TF Data API has simplified this pipeline process by using chaining. So, we can simply fit like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVnufDmf9-mr"
   },
   "outputs": [],
   "source": [
    "... = model.fit(train_dataset.batch(batch_size).map(decode).prefetch(1),...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMJ6dgonZWv3"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Along side TF-Records once the TF Data API is tuned we got performance improvement around 3.75-4x times. Just plain TF-Records was providing plain 3x times performance improvement on tabular data. \n",
    "\n",
    "We could see blogs such as [these](https://sebastianwallkoetter.wordpress.com/2018/02/24/optimize-tf-input-pipeline/) by *Sebastian Wallk√∂tter* which claim 7x improvement in Image dataset.\n",
    "\n",
    "Based on my understanding, the reasons on why I couldnt achieve that improvement are as follows:\n",
    "- Data was stored in SSD (both CSV and TF-Records) which by itself does faster reads. Hence the impact of TF-Records being read faster became less predominant\n",
    "- Training time for tabular batch are typically slow compared to image data as tabular data has fewer features. (making our performance bottle neck CPU bound)\n",
    "\n",
    "\n",
    "We could try to distribute training using a distribution strategy available in tensorflow in spark. Before distribution, we can consider if the issue is CPU/GPU bound (i.e) the performance bottle neck was on the time taken to read the data or train the model. \n",
    "\n",
    "For instance, GPU which does the training might be waiting for the CPU to get the training data for each batch, if the training finishes before retrieving the data for the next batch. This makes the performance bottle-neck *CPU bound*. \n",
    "\n",
    "If the training take more time than the time to retrieve the data, we might have CPU wait for GPU to complete the training. This makes the problem *GPU bound*.\n",
    "\n",
    "This idea of wheather an issue is CPU or GPU bound gives us idea on what further course of action can be done to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "No7YZO6WBvyZ"
   },
   "source": [
    "## Useful Sources:\n",
    "- [Spark TensorFlow Connector](https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector)\n",
    "- [Official Documentation](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n",
    "- [TF Records Good on Keras TensorFlow discussion on Quora](https://www.quora.com/Is-it-especially-good-to-use-tfRecord-as-input-data-format-if-I-am-using-Keras-Tensorflow)\n",
    "- [7x speedup with an optimized TensorFlow Input pipeline: TFRecords + Dataset API](https://sebastianwallkoetter.wordpress.com/2018/02/24/optimize-tf-input-pipeline/)\n",
    "- [Tensorflow Records? What they are and how to use them](https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6HkB1yvKaAq"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Kindly share your experience and perspectives on training with TF-Records and on spark clusters"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TF Records on Spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
